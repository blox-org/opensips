From a9de042cd807ed97233ebca3f4632031157ff742 Mon Sep 17 00:00:00 2001
From: Varadhan M <varadhan@cem-solutions.net>
Date: Thu, 22 Mar 2018 13:34:07 +0000
Subject: Upgraded cachedb_mongodb2 to support mongo 2.4+

---
 db/db.c                                            |    6 +-
 db/db_id.c                                         |   10 +-
 db/db_res.c                                        |   40 +-
 db/db_res.h                                        |    3 +
 db/db_row.c                                        |   13 +
 db/db_row.h                                        |    2 +
 db/db_val.h                                        |   33 +-
 modules/cachedb_mongodb2/Makefile                  |   42 +
 modules/cachedb_mongodb2/README                    |  234 +++
 modules/cachedb_mongodb2/cachedb_mongodb.c         |  171 ++
 modules/cachedb_mongodb2/cachedb_mongodb_dbase.c   | 1645 ++++++++++++++++++++
 modules/cachedb_mongodb2/cachedb_mongodb_dbase.h   |   95 ++
 modules/cachedb_mongodb2/cachedb_mongodb_json.c    |  272 ++++
 modules/cachedb_mongodb2/cachedb_mongodb_json.h    |   35 +
 modules/cachedb_mongodb2/doc/cachedb_mongodb.xml   |   48 +
 .../cachedb_mongodb2/doc/cachedb_mongodb_admin.xml |  327 ++++
 16 files changed, 2970 insertions(+), 6 deletions(-)
 create mode 100644 modules/cachedb_mongodb2/Makefile
 create mode 100644 modules/cachedb_mongodb2/README
 create mode 100644 modules/cachedb_mongodb2/cachedb_mongodb.c
 create mode 100644 modules/cachedb_mongodb2/cachedb_mongodb_dbase.c
 create mode 100644 modules/cachedb_mongodb2/cachedb_mongodb_dbase.h
 create mode 100644 modules/cachedb_mongodb2/cachedb_mongodb_json.c
 create mode 100644 modules/cachedb_mongodb2/cachedb_mongodb_json.h
 create mode 100644 modules/cachedb_mongodb2/doc/cachedb_mongodb.xml
 create mode 100644 modules/cachedb_mongodb2/doc/cachedb_mongodb_admin.xml

diff --git a/db/db.c b/db/db.c
index f658a0b..0da86b7 100644
--- a/db/db.c
+++ b/db/db.c
@@ -244,7 +244,10 @@ int db_bind_mod(const str* mod, db_func_t* mydbf)
 		dbf.insert_update = (db_insert_update_f)find_mod_export(tmp,
 			"db_insert_update", 2, 0);
 	}
-	if(db_check_api(&dbf, tmp)!=0)
+	/* check if the module pre-populated the capabilities, or we need to
+	 * compute them ourselves - we check for the INSERT capability, because
+	 * it's the only one that should be exported by all modules */
+	if(!DB_CAPABILITY(dbf, DB_CAP_INSERT) && db_check_api(&dbf, tmp)!=0)
 		goto error;
 
 	*mydbf=dbf; /* copy */
@@ -314,6 +317,7 @@ db_con_t* db_do_init(const str* url, void* (*new_connection)())
 		LM_DBG("connection %p inserted in pool as %p\n", id,con);
 	} else {
 		LM_DBG("connection %p found in pool as %p\n", id,con);
+		free_db_id(id);
 	}
 
 	if (!con->transfers) {
diff --git a/db/db_id.c b/db/db_id.c
index 7179c21..88423a0 100644
--- a/db/db_id.c
+++ b/db/db_id.c
@@ -40,6 +40,8 @@
  */
 static int dupl_string(char** dst, const char* begin, const char* end)
 {
+	str old, new;
+
 	if (*dst) pkg_free(*dst);
 
 	*dst = pkg_malloc(end - begin + 1);
@@ -47,8 +49,12 @@ static int dupl_string(char** dst, const char* begin, const char* end)
 		return -1;
 	}
 
-	memcpy(*dst, begin, end - begin);
-	(*dst)[end - begin] = '\0';
+	old.s = (char*)begin;
+	old.len = end - begin;
+	new.s = *dst;
+	un_escape(&old, &new );
+
+	new.s[new.len] = '\0';
 	return 0;
 }
 
diff --git a/db/db_res.c b/db/db_res.c
index 88af5fd..b76e9db 100644
--- a/db/db_res.c
+++ b/db/db_res.c
@@ -87,7 +87,8 @@ int db_free_columns(db_res_t* _r)
  */
 db_res_t* db_new_result(void)
 {
-	db_res_t* r = NULL;
+	db_res_t* r;
+
 	r = (db_res_t*)pkg_malloc(sizeof(db_res_t));
 	if (!r) {
 		LM_ERR("no private memory left\n");
@@ -114,7 +115,6 @@ int db_free_result(db_res_t* _r)
 	db_free_rows(_r);
 	LM_DBG("freeing result set at %p\n", _r);
 	pkg_free(_r);
-	_r = NULL;
 	return 0;
 }
 
@@ -173,10 +173,46 @@ int db_allocate_rows(db_res_t* _res, const unsigned int rows)
 	return 0;
 }
 
+/*
+ * Extend storage for rows in existing result structure.
+ */
+int db_realloc_rows(db_res_t *_res, const unsigned int old_rows,
+                    const unsigned int rows)
+{
+	unsigned int i;
+	struct db_row *old_buf;
 
+	old_buf = RES_ROWS(_res);
 
+	RES_ROWS(_res) = pkg_malloc(rows * (sizeof(db_row_t) +
+	                                    sizeof(db_val_t) * RES_COL_N(_res)) );
+	if (!RES_ROWS(_res)) {
+		RES_ROWS(_res) = old_buf;
+		LM_ERR("no memory left\n");
+		return -1;
+	}
 
+	memset(RES_ROWS(_res), 0,
+	       rows * (sizeof(db_row_t) + sizeof(db_val_t) * RES_COL_N(_res)));
 
+	memcpy(RES_ROWS(_res), old_buf, old_rows * sizeof(db_row_t));
+	memcpy(RES_ROWS(_res) + rows,
+	       (char *)old_buf + old_rows * sizeof(db_row_t),
+	       old_rows * (sizeof(db_val_t) * RES_COL_N(_res)));
 
+	LM_DBG("%p:%d allocate %d bytes for result rows and values at %p\n",
+		old_buf, old_rows, (int)(rows * (sizeof(db_row_t) + sizeof(db_val_t) * RES_COL_N(_res))),
+		RES_ROWS(_res));
+
+	if(old_buf) {
+		pkg_free(old_buf);
+	}
 
 
+	for( i=0 ; i<rows ; i++ )
+		/* the values of the row i */
+		ROW_VALUES( &(RES_ROWS(_res)[i]) ) =
+			((db_val_t*)(RES_ROWS(_res)+rows)) + RES_COL_N(_res)*i;
+
+	return 0;
+}
diff --git a/db/db_res.h b/db/db_res.h
index 97c12d3..fd50c8f 100644
--- a/db/db_res.h
+++ b/db/db_res.h
@@ -120,4 +120,7 @@ int db_allocate_columns(db_res_t* _r, const unsigned int cols);
 
 int db_allocate_rows(db_res_t* _res, const unsigned int rows);
 
+int db_realloc_rows(db_res_t *_res, const unsigned int old_rows,
+                    const unsigned int rows);
+
 #endif /* DB_RES_H */
diff --git a/db/db_row.c b/db/db_row.c
index 14f111b..d7c63a5 100644
--- a/db/db_row.c
+++ b/db/db_row.c
@@ -33,6 +33,19 @@
 #include "../dprint.h"
 #include "../mem/mem.h"
 
+void db_print_rows(db_row_t *_r, int count)
+{
+	int ri, c;
+
+	for (ri = 0; ri < count; ri++) {
+		LM_DBG("%d cols:\n", _r[ri].n);
+		for (c = 0; c < _r[ri].n; c++) {
+			LM_DBG("it\n");
+			db_print_val(&_r[ri].values[c]);
+		}
+	}
+}
+
 /*
  * Release memory used by row
  */
diff --git a/db/db_row.h b/db/db_row.h
index 626b77d..19e3656 100644
--- a/db/db_row.h
+++ b/db/db_row.h
@@ -50,6 +50,8 @@ typedef struct db_row {
 /** Return the number of colums */
 #define ROW_N(rw)      ((rw)->n)
 
+void db_print_rows(db_row_t *_r, int count);
+
 /**
  * Release memory used by a row. This method only frees values that are inside
  * the row if the free flag of the specific value is set. Otherwise this
diff --git a/db/db_val.h b/db/db_val.h
index b9ade99..28da9b4 100644
--- a/db/db_val.h
+++ b/db/db_val.h
@@ -35,6 +35,8 @@
 
 #include <time.h>
 #include <stdint.h>
+
+#include "../dprint.h"
 #include "../str.h"
 
 
@@ -87,6 +89,36 @@ typedef struct {
 	} val;
 } db_val_t;
 
+static inline void db_print_val(db_val_t *v)
+{
+	switch (v->type) {
+		case DB_INT:
+			LM_GEN1(L_DBG, "\t'%d'\n", v->val.int_val);
+			break;
+		case DB_BIGINT:
+			LM_GEN1(L_DBG, "\t'%lld'\n", v->val.bigint_val);
+			break;
+		case DB_DOUBLE:
+			LM_GEN1(L_DBG, "\t'%.3lf'\n", v->val.double_val);
+			break;
+		case DB_STRING:
+			LM_GEN1(L_DBG, "\t'%s'\n", v->val.string_val);
+			break;
+		case DB_STR:
+			LM_GEN1(L_DBG, "\t'%.*s'\n", v->val.str_val.len, v->val.str_val.s);
+			break;
+		case DB_DATETIME:
+			LM_GEN1(L_DBG, "\t'%ld'\n", v->val.time_val);
+			break;
+		case DB_BLOB:
+			LM_GEN1(L_DBG, "\t'%.*s'\n", v->val.blob_val.len, v->val.blob_val.s);
+			break;
+		case DB_BITMAP:
+			LM_GEN1(L_DBG, "\t'%u'\n", v->val.bitmap_val);
+			break;
+	}
+}
+
 
 /**
  * Useful macros for accessing attributes of db_val structure.
@@ -163,7 +195,6 @@ typedef struct {
 #define VAL_BITMAP(dv) ((dv)->val.bitmap_val)
 
 
-
 #define get_str_from_dbval( _col_name, _val, _not_null, _not_empty, _str, _error_label) \
 	do{\
 		if ((_val)->nul) { \
diff --git a/modules/cachedb_mongodb2/Makefile b/modules/cachedb_mongodb2/Makefile
new file mode 100644
index 0000000..578eb7b
--- /dev/null
+++ b/modules/cachedb_mongodb2/Makefile
@@ -0,0 +1,42 @@
+#
+# MongoDB C client
+# 
+# WARNING: do not run this directly, it should be run by the master Makefile
+
+include ../../Makefile.defs
+auto_gen=
+NAME=cachedb_mongodb2.so
+
+ifeq ($(CROSS_COMPILE),)
+JSON_BUILDER = $(shell \
+	if pkg-config --exists json; then \
+		echo 'pkg-config json'; \
+	elif pkg-config --exists json-c; then\
+		echo 'pkg-config json-c'; \
+	fi)
+endif
+
+ifeq ($(JSON_BUILDER),)
+	DEFS += -I$(LOCALBASE)/include -I$(SYSBASE)/include
+	LIBS += -L$(LOCALBASE)/lib -ljson
+else
+	DEFS += $(shell $(JSON_BUILDER) --cflags)
+	LIBS += $(shell $(JSON_BUILDER) --libs)
+endif
+
+ifeq ($(CROSS_COMPILE),)
+MONGOC_BUILDER = $(shell \
+	if pkg-config --exists libmongoc-1.0; then \
+		echo 'pkg-config libmongoc-1.0'; \
+	fi)
+endif
+
+ifeq ($(MONGOC_BUILDER),)
+	DEFS += -I$(SYSBASE)/include/libmongoc-1.0 -I$(SYSBASE)/include/libbson-1.0
+	LIBS += -L$(LOCALBASE)/lib -lssl -lcrypto -lrt -lmongoc-1.0 -lbson-1.0
+else
+	DEFS += $(shell $(MONGOC_BUILDER) --cflags)
+	LIBS += $(shell $(MONGOC_BUILDER) --libs)
+endif
+
+include ../../Makefile.modules
diff --git a/modules/cachedb_mongodb2/README b/modules/cachedb_mongodb2/README
new file mode 100644
index 0000000..69ac1ab
--- /dev/null
+++ b/modules/cachedb_mongodb2/README
@@ -0,0 +1,234 @@
+cachedb_mongodb Module
+
+Vladut-Stefan Paiu
+
+   OpenSIPS Solutions
+
+Edited by
+
+Vladut-Stefan Paiu
+
+   Copyright Â© 2013-2017 www.opensips-solutions.com
+     __________________________________________________________
+
+   Table of Contents
+
+   1. Admin Guide
+
+        1.1. Overview
+        1.2. Advantages
+        1.3. Limitations
+        1.4. Dependencies
+
+              1.4.1. OpenSIPS Modules
+              1.4.2. External Libraries or Applications
+
+        1.5. Exported Parameters
+
+              1.5.1. cachedb_url (string)
+              1.5.2. exec_threshold (int)
+              1.5.3. compat_mode_2.4 (int)
+              1.5.4. compat_mode_3.0 (int)
+
+        1.6. Exported Functions
+        1.7. Raw Query Syntax
+
+   List of Examples
+
+   1.1. Runtime requirements for "cachedb_mongodb"
+   1.2. Compilation requirements for "cachedb_mongodb"
+   1.3. Set cachedb_url parameter
+   1.4. Use MongoDB servers
+   1.5. Set exec_threshold parameter
+   1.6. Setting the compat_mode_2.4 parameter
+   1.7. Setting the compat_mode_3.0 parameter
+   1.8. MongoDB Raw Insert
+   1.9. MongoDB Raw Update
+
+Chapter 1. Admin Guide
+
+1.1. Overview
+
+   This module is an implementation of a cache system designed to
+   work with MongoDB servers. It implements the Key-Value
+   interface exposed by the OpenSIPS core.
+
+   The underlying client library is compatible with any of the
+   following MongoDB server versions: 2.4, 2.6, 3.0, 3.2 and 3.4,
+   as stated in the MongoDB documentation.
+
+1.2. Advantages
+
+     * memory costs are no longer on the server
+     * many servers can be used inside a cluster, so the memory is
+       virtually unlimited
+     * the cache is 100% persistent. A restart of OpenSIPS server
+       will not affect the DB. The MongoDB is also persistent so
+       it can also be restarted without loss of information.
+     * MongoDB is an open-source project so it can be used to
+       exchange data with various other applications
+     * By creating a MongoDB Cluster, multiple OpenSIPS instances
+       can easily share key-value information
+     * This module also implements the CacheDB Raw query
+       capability, thus you can run whatever query that the
+       MongoDB back-end supports, taking full advatange of it.
+
+1.3. Limitations
+
+     * keys (in key:value pairs) may not contain spaces or control
+       characters
+
+1.4. Dependencies
+
+1.4.1. OpenSIPS Modules
+
+   None.
+
+1.4.2. External Libraries or Applications
+
+   The following packages must be installed before running
+   OpenSIPS with this module loaded:
+
+   Example 1.1. Runtime requirements for "cachedb_mongodb"
+# Debian / Ubuntu
+sudo apt-get install libjson-c2 libmongoc-1.0
+
+# Red Hat / CentOS
+sudo yum install json-c mongo-c-driver
+
+   The following packages are required in order to compile this
+   module:
+
+   Example 1.2. Compilation requirements for "cachedb_mongodb"
+# Debian / Ubuntu
+sudo apt-get install libjson-c-dev libmongoc-dev libbson-dev
+
+# Red Hat / CentOS
+sudo yum install json-c-devel mongo-c-driver-devel
+
+1.5. Exported Parameters
+
+1.5.1. cachedb_url (string)
+
+   The URLs of the server groups that OpenSIPS will connect to in
+   order to allow the cache_store(), cache_fetch(), etc. functions
+   to be used from the OpenSIPS script. It can be set more than
+   one time. The prefix part of the URL will be the identifier
+   that will be used from the script.
+
+   The URL syntax is identical to the one used by MongoDB,
+   including connect string options. For more info, please refer
+   to the official MongoDB connect string documentation.
+
+   Example 1.3. Set cachedb_url parameter
+...
+modparam("cachedb_mongodb", "cachedb_url","mongodb:instance1://localhost
+:27017/db.collection")
+modparam("cachedb_mongodb", "cachedb_url","mongodb:replicaset1://1.2.3.4
+:27017,2.3.4.5:27017,3.4.5.6:27017/db.collection?replicaSet=test")
+...
+
+   Example 1.4. Use MongoDB servers
+...
+cache_store("mongodb:group1", "key", "$ru value");
+cache_fetch("mongodb:replicaset1", "key", $avp(10));
+cache_remove("mongodb:cluster1", "key");
+...
+
+1.5.2. exec_threshold (int)
+
+   The maximum number of microseconds that a mongodb query can
+   last. Anything above the threshold will trigger a warning
+   message to the log
+
+   Default value is â0 ( unlimited - no warnings )â.
+
+   Example 1.5. Set exec_threshold parameter
+...
+modparam("cachedb_mongodb", "exec_threshold", 100000)
+...
+
+1.5.3. compat_mode_2.4 (int)
+
+   Switch the module into compatibility mode for MongoDB 2.4
+   servers. Specifically, this allows "insert/update/delete" raw
+   queries to not fail, since they were introduced in MongoDB 2.6.
+   The module will interpret the raw query JSON, convert it to its
+   corresponding command and run it.
+
+   Caveat: only the minimally required raw query options are
+   supported in this mode.
+
+   Default value is â0 (disabled)â.
+
+   Example 1.6. Setting the compat_mode_2.4 parameter
+...
+modparam("cachedb_mongodb", "compat_mode_2.4", 1)
+...
+
+1.5.4. compat_mode_3.0 (int)
+
+   Switch the module into compatibility mode for MongoDB 2.6/3.0
+   servers. Specifically, this allows "find" raw queries to not
+   fail, since they were introduced in MongoDB 3.2. The module
+   will interpret the "find" raw query JSON, convert it to its
+   corresponding command and run it.
+
+   Caveat: only the minimally required options for "find" raw
+   queries are supported in this mode.
+
+   Default value is â0 (disabled)â.
+
+   Example 1.7. Setting the compat_mode_3.0 parameter
+...
+modparam("cachedb_mongodb", "compat_mode_3.0", 1)
+...
+
+1.6. Exported Functions
+
+   The module does not export functions to be used in
+   configuration script.
+
+1.7. Raw Query Syntax
+
+   The cachedb_mongodb module supports raw queries, thus taking
+   full advantage of the capabilities of the back-end, including
+   query-specific options such as read/write preference, timeouts,
+   filtering options, etc.
+
+   The query syntax is identical to the mongo cli. Documentation
+   for it can be found on the MongoDB website. Query results are
+   returned as JSON documents, that one can further process in the
+   OpenSIPS script by using the JSON module.
+
+   Some example raw queries:
+
+   Example 1.8. MongoDB Raw Insert
+...
+cache_raw_query("mongodb:cluster", "{ \
+    \"insert\": \"ip_blacklist\", \
+    \"documents\": [{ \
+        \"username\": \"$fU\", \
+        \"ip\": \"$si\", \
+        \"attempts\": 1 \
+     }]}",
+ "$avp(out)");
+xlog("INSERT RAW QUERY returned $rc, output: '$avp(out)'\n");
+...
+
+   Example 1.9. MongoDB Raw Update
+...
+cache_raw_query("mongodb:cluster", "{ \
+    \"update\": \"ip_blacklist\", \
+    \"updates\": [{ \
+        \"q\": { \
+            \"username\": \"$fU\", \
+            \"ip\": \"$si\" \
+         }, \
+        \"u\": { \
+            \"$$inc\": {\"attempts\": 1} \
+         } \
+      }]}",
+ "$avp(out)");
+xlog("UPDATE RAW QUERY returned $rc, output: '$avp(out)'\n");
+...
diff --git a/modules/cachedb_mongodb2/cachedb_mongodb.c b/modules/cachedb_mongodb2/cachedb_mongodb.c
new file mode 100644
index 0000000..88269c9
--- /dev/null
+++ b/modules/cachedb_mongodb2/cachedb_mongodb.c
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2011-2017 OpenSIPS Project
+ *
+ * This file is part of opensips, a free SIP server.
+ *
+ * opensips is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * opensips is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+ */
+
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <sys/types.h>
+#include <sys/ipc.h>
+#include <unistd.h>
+#include <fcntl.h>
+#include <time.h>
+
+#include "../../sr_module.h"
+#include "../../dprint.h"
+#include "../../error.h"
+#include "../../pt.h"
+#include "../../cachedb/cachedb.h"
+
+#include "cachedb_mongodb_dbase.h"
+#include "cachedb_mongodb_json.h"
+
+static int mod_init(void);
+static int child_init(int);
+static void destroy(void);
+
+static str cache_mod_name = str_init("mongodb");
+struct cachedb_url *mongodb_script_urls = NULL;
+
+int mongo_exec_threshold=0;
+
+int compat_mode_30;
+int compat_mode_24;
+
+int set_connection(unsigned int type, void *val)
+{
+	return cachedb_store_url(&mongodb_script_urls,(char *)val);
+}
+
+static param_export_t params[]={
+	{ "cachedb_url",   STR_PARAM|USE_FUNC_PARAM, (void *)&set_connection},
+	{ "exec_treshold", INT_PARAM, &mongo_exec_threshold },
+	{ "compat_mode_3.0", INT_PARAM, &compat_mode_30 },
+	{ "compat_mode_2.4", INT_PARAM, &compat_mode_24 },
+	{0,0,0}
+};
+
+static dep_export_t deps = {
+	{ /* OpenSIPS module dependencies */
+
+		/* tls_mgm must init TLS first, since it also sets custom alloc func */
+		{ MOD_TYPE_DEFAULT, "tls_mgm", DEP_SILENT },
+		{ MOD_TYPE_NULL, NULL, 0 },
+	},
+	{ /* modparam dependencies */
+		{ NULL, NULL },
+	},
+};
+
+/** module exports */
+struct module_exports exports= {
+	"cachedb_mongodb2",					/* module name */
+	MOD_TYPE_CACHEDB,/* class of this module */
+	MODULE_VERSION,
+	DEFAULT_DLFLAGS,			/* dlopen flags */
+	&deps,            /* OpenSIPS module dependencies */
+	0,						/* exported functions */
+	0,						/* exported async functions */
+	params,						/* exported parameters */
+	0,							/* exported statistics */
+	0,							/* exported MI functions */
+	0,							/* exported pseudo-variables */
+	0,							/* extra processes */
+	mod_init,					/* module initialization function */
+	(response_function) 0,      /* response handling function */
+	(destroy_function)destroy,	/* destroy function */
+	child_init			        /* per-child init function */
+};
+
+
+/**
+ * init module function
+ */
+static int mod_init(void)
+{
+	cachedb_engine cde;
+
+	mongoc_init();
+
+	LM_NOTICE("initializing module cachedb_mongodb2 ...\n");
+	memset(&cde,0,sizeof(cachedb_engine));
+
+	cde.name = cache_mod_name;
+
+	cde.cdb_func.init = mongo_con_init;
+	cde.cdb_func.destroy = mongo_con_destroy;
+	cde.cdb_func.get = mongo_con_get;
+	cde.cdb_func.get_counter = mongo_con_get_counter;
+	cde.cdb_func.set = mongo_con_set;
+	cde.cdb_func.remove = mongo_con_remove;
+	cde.cdb_func.add = mongo_con_add;
+	cde.cdb_func.sub = mongo_con_sub;
+	cde.cdb_func.raw_query = mongo_con_raw_query;
+	cde.cdb_func.db_query_trans = mongo_db_query_trans;
+	cde.cdb_func.db_free_trans = mongo_db_free_result_trans;
+	cde.cdb_func.db_insert_trans = mongo_db_insert_trans;
+	cde.cdb_func.db_delete_trans = mongo_db_delete_trans;
+	cde.cdb_func.db_update_trans = mongo_db_update_trans;
+
+	cde.cdb_func.capability = 0;
+
+	if (register_cachedb(&cde) < 0) {
+		LM_ERR("failed to initialize cachedb_mongodb2\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static int child_init(int rank)
+{
+	struct cachedb_url *it;
+	cachedb_con *con;
+
+	if(rank == PROC_MAIN || rank == PROC_TCP_MAIN) {
+		return 0;
+	}
+
+	for (it = mongodb_script_urls;it;it=it->next) {
+		LM_DBG("iterating through conns - [%.*s]\n",it->url.len,it->url.s);
+		con = mongo_con_init(&it->url);
+		if (con == NULL) {
+			LM_ERR("failed to open connection\n");
+			return -1;
+		}
+		if (cachedb_put_connection(&cache_mod_name,con) < 0) {
+			LM_ERR("failed to insert connection\n");
+			return -1;
+		}
+	}
+
+	cachedb_free_url(mongodb_script_urls);
+	return 0;
+}
+
+/*
+ * destroy function
+ */
+static void destroy(void)
+{
+	LM_NOTICE("destroy module cachedb_mongodb2 ...\n");
+
+	cachedb_end_connections(&cache_mod_name);
+	mongoc_cleanup();
+}
diff --git a/modules/cachedb_mongodb2/cachedb_mongodb_dbase.c b/modules/cachedb_mongodb2/cachedb_mongodb_dbase.c
new file mode 100644
index 0000000..11a250a
--- /dev/null
+++ b/modules/cachedb_mongodb2/cachedb_mongodb_dbase.c
@@ -0,0 +1,1645 @@
+/*
+ * Copyright (C) 2011-2017 OpenSIPS Project
+ *
+ * This file is part of opensips, a free SIP server.
+ *
+ * opensips is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * opensips is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+ */
+
+#include "../../dprint.h"
+#include "cachedb_mongodb_dbase.h"
+#include "cachedb_mongodb_json.h"
+#include "../../mem/mem.h"
+#include "../../ut.h"
+#include "../../pt.h"
+#include "../../cachedb/cachedb.h"
+
+#include <string.h>
+
+extern str mongo_write_concern_str;
+extern str mongo_write_concern_b;
+extern int mongo_slave_ok;
+extern int mongo_exec_threshold;
+
+extern int compat_mode_30;
+extern int compat_mode_24;
+
+#define HEX_OID_SIZE 25
+char *hex_oid_id;
+
+/**
+ * Builds a MongoDB connect string URI of the form:
+ *
+ * mongodb://[username:password@]
+ *	   host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]
+ *
+ * See https://docs.mongodb.com/manual/reference/connection-string
+ */
+static char *build_mongodb_connect_string(struct cachedb_id *id)
+{
+	char *ret, *p;
+	int len;
+
+	len =
+	      strlen(id->scheme) + 3 +
+	      (id->username ? strlen(id->username) : 0) + 1 +
+	      (id->password ? strlen(id->password) : 0) + 1 +
+	      strlen(id->host) + 1 +
+		  5 + 1 + /* port */
+	      strlen(id->database) + 1 +
+		  1;
+
+	ret = pkg_malloc(len);
+	if (!ret) {
+		LM_ERR("oom\n");
+		return NULL;
+	}
+
+	p = memchr(id->database, '.', strlen(id->database));
+
+	if (id->username && id->password) {
+		if (id->port == 0) {
+			sprintf(ret, "mongodb://%s:%s@%s/%s", id->username, id->password,
+			        id->host, id->database);
+		} else {
+			sprintf(ret, "mongodb://%s:%s@%s:%d/%s", id->username, id->password,
+			        id->host, id->port, id->database);
+		}
+
+	} else {
+		if (id->port == 0) {
+			sprintf(ret, "mongodb://%s/%.*s", id->host,
+			        (int)(p ? p - id->database : strlen(id->database)), id->database);
+		} else {
+			sprintf(ret, "mongodb://%s:%d/%.*s", id->host, id->port,
+			        (int)(p ? p - id->database : strlen(id->database)), id->database);
+		}
+	}
+
+	return ret;
+}
+
+#ifndef MONGOC_HANDSHAKE_APPNAME_MAX
+#define MONGOC_HANDSHAKE_APPNAME_MAX 128
+#endif
+
+char osips_appname[MONGOC_HANDSHAKE_APPNAME_MAX];
+
+static inline char *pkg_strdup(const char *str)
+{
+        char *rval;
+        int len;
+
+        if (!str)
+                return NULL;
+
+        len = strlen(str) + 1;
+        rval = pkg_malloc(len);
+        if (!rval)
+                return NULL;
+        memcpy(rval, str, len); 
+        return rval; 
+}
+
+mongo_con* mongo_new_connection(struct cachedb_id* id)
+{
+	char *p, *conn_str;
+	mongo_con *con;
+
+	snprintf(osips_appname, MONGOC_HANDSHAKE_APPNAME_MAX, "opensips-%d", my_pid());
+
+	LM_DBG("MongoDB conn for [%s]: %s:%s %s:%s |%s|:%u\n", osips_appname,
+	       id->scheme, id->group_name, id->username, id->password, id->host, id->port);
+
+	conn_str = build_mongodb_connect_string(id);
+
+	LM_DBG("cstr: %s\n", conn_str);
+
+	con = pkg_malloc(sizeof *con);
+	if (!con) {
+		LM_ERR("oom!\n");
+		return NULL;
+	}
+	memset(con, 0, sizeof *con);
+	con->id = id;
+	con->ref = 1;
+
+	con->client = mongoc_client_new(conn_str);
+	if (!con->client) {
+		LM_ERR("failed to connect to Mongo (%s)\n", conn_str);
+		return NULL;
+	}
+
+	p = memchr(id->database, '.', strlen(id->database));
+	if (!p) {
+		LM_ERR("malformed Mongo database part in %s\n", id->database);
+		return NULL;
+	}
+
+	*p = '\0';
+	con->db = pkg_strdup(id->database);
+	con->col = pkg_strdup(p + 1);
+	if (!con->db || !con->col) {
+		LM_ERR("oom\n");
+		return NULL;
+	}
+
+	con->database = mongoc_client_get_database(con->client, id->database);
+	con->collection = mongoc_client_get_collection(con->client, id->database, p+1);
+	*p = '.';
+
+	pkg_free(conn_str);
+	return con;
+}
+
+cachedb_con *mongo_con_init(str *url)
+{
+	return cachedb_do_init(url,(void *)mongo_new_connection);
+}
+
+void mongo_free_connection(cachedb_pool_con *con)
+{
+	mongo_con *mcon = (mongo_con *)con;
+
+	mongoc_collection_destroy(mcon->collection);
+	mongoc_database_destroy(mcon->database);
+	mongoc_client_destroy(mcon->client);
+	pkg_free(mcon->db);
+	pkg_free(mcon->col);
+}
+
+void mongo_con_destroy(cachedb_con *con)
+{
+	LM_DBG("in mongo_destroy\n");
+	cachedb_do_close(con,mongo_free_connection);
+}
+
+int mongo_con_get(cachedb_con *con, str *attr, str *val)
+{
+	bson_t *filter;
+	mongoc_cursor_t *cursor;
+	const bson_t *doc;
+	bson_iter_t iter;
+	const bson_value_t *value;
+	struct timeval start;
+	unsigned long ival;
+	char *p;
+
+	LM_DBG("find %.*s in %s\n", attr->len, attr->s,
+	       MONGO_NAMESPACE(con));
+
+	filter = bson_new();
+#define MONGOC_CHECK_VERSION(major,minor,micro)   \
+        (MONGOC_MAJOR_VERSION > (major) || \
+         (MONGOC_MAJOR_VERSION == (major) && MONGOC_MINOR_VERSION > (minor)) || \
+         (MONGOC_MAJOR_VERSION == (major) && MONGOC_MINOR_VERSION == (minor) && \
+          MONGOC_MICRO_VERSION >= (micro)))
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	bson_append_utf8(filter, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find_with_opts(
+	                MONGO_COLLECTION(con), filter, NULL, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB get",
+	                  attr->s, attr->len, 0);
+
+	while (mongoc_cursor_next(cursor, &doc)) {
+#else
+	bson_t child;
+	BSON_APPEND_DOCUMENT_BEGIN(filter, "$query", &child);
+	bson_append_utf8(&child, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+	bson_append_document_end(filter, &child);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find(MONGO_COLLECTION(con), MONGOC_QUERY_NONE,
+	                                0, 0, 0, filter, NULL, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB get",
+	                  attr->s, attr->len, 0);
+
+	while (mongoc_cursor_more(cursor) && mongoc_cursor_next(cursor, &doc)) {
+#endif
+		if (bson_iter_init_find(&iter, doc, "opensips")) {
+			value = bson_iter_value(&iter);
+			switch (value->value_type) {
+			case BSON_TYPE_UTF8:
+				val->len = value->value.v_utf8.len;
+				val->s = pkg_malloc(val->len);
+					if (!val->s) {
+						LM_ERR("oom!\n");
+					goto out_err;
+				}
+				memcpy(val->s, value->value.v_utf8.str, val->len);
+				goto out;
+			case BSON_TYPE_INT32:
+				ival = (unsigned long)value->value.v_int32;
+				break;
+			case BSON_TYPE_INT64:
+				ival = (unsigned long)value->value.v_int64;
+				break;
+			default:
+				LM_ERR("unsupported type %d for key %.*s!\n",
+				       value->value_type, attr->len, attr->s);
+				goto out_err;
+			}
+
+			p = int2str(ival, &val->len);
+			val->s = pkg_malloc(val->len);
+			if (!val->s) {
+				LM_ERR("oom!\n");
+				goto out_err;
+			}
+			memcpy(val->s, p, val->len);
+			goto out;
+		}
+	}
+
+	LM_DBG("key not found: %.*s\n", attr->len, attr->s);
+
+out_err:
+	bson_destroy(filter);
+	mongoc_cursor_destroy(cursor);
+	memset(val, 0, sizeof *val);
+	return -1;
+
+out:
+	bson_destroy(filter);
+	mongoc_cursor_destroy(cursor);
+	return 0;
+}
+
+int mongo_con_set(cachedb_con *con, str *attr, str *val, int expires)
+{
+	bson_t *query, *update;
+	bson_t child;
+	bson_error_t error;
+	struct timeval start;
+	int ret = 0;
+
+	query = bson_new();
+	bson_append_utf8(query, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+
+	update = bson_new();
+	BSON_APPEND_DOCUMENT_BEGIN(update, "$set", &child);
+	bson_append_utf8(&child, "opensips", 8, val->s, val->len);
+	bson_append_document_end(update, &child);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_update(MONGO_COLLECTION(con), MONGOC_UPDATE_UPSERT,
+	                              query, update, NULL, &error)) {
+		LM_ERR("failed to store %.*s=%.*s\n",
+		       attr->len, attr->s, val->len, val->s);
+		ret = -1;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB set",
+	                  attr->s, attr->len, 0);
+
+	bson_destroy(query);
+	bson_destroy(update);
+
+	return ret;
+}
+
+int mongo_con_remove(cachedb_con *con, str *attr)
+{
+	bson_t *doc;
+	bson_error_t error;
+	struct timeval start;
+	int ret = 0;
+
+	doc = bson_new();
+	bson_append_utf8(doc, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_remove(MONGO_COLLECTION(con),
+	                         MONGOC_REMOVE_SINGLE_REMOVE, doc, NULL, &error)) {
+		LM_ERR("failed to remove key '%.*s'\n", attr->len, attr->s);
+		ret = -1;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB remove",
+	                  attr->s, attr->len, 0);
+
+	bson_destroy(doc);
+
+	return ret;
+}
+
+int mongo_raw_find(cachedb_con *con, bson_t *raw_query, bson_iter_t *ns,
+                   cdb_raw_entry ***reply, int expected_kv_no, int *reply_no)
+{
+	struct json_object *obj = NULL;
+	mongoc_collection_t *col = NULL;
+	bson_iter_t iter;
+	bson_t _query, *query = NULL, *opts = NULL, proj;
+	mongoc_cursor_t *cursor;
+	struct timeval start;
+	const bson_value_t *v;
+	const bson_t *doc;
+	int i, len, csz = 0, ret = -1;
+	const char *p;
+
+	if (bson_iter_type(ns) != BSON_TYPE_UTF8) {
+		LM_ERR("collection name must be a string (%d)!\n", bson_iter_type(ns));
+		return -1;
+	}
+
+	*reply = NULL;
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   bson_iter_utf8(ns, NULL));
+
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	if (bson_iter_init_find(&iter, raw_query, "filter") &&
+	    BSON_ITER_HOLDS_DOCUMENT(&iter)) {
+		v = bson_iter_value(&iter);
+		bson_init_static(&_query, v->value.v_doc.data, v->value.v_doc.data_len);
+	} else {
+		bson_init(&_query);
+	}
+	query = &_query;
+#else
+	bson_t *fields = NULL, child;
+
+	query = bson_new();
+	BSON_APPEND_DOCUMENT_BEGIN(query, "$query", &child);
+	if (bson_iter_init_find(&iter, raw_query, "filter") &&
+	    BSON_ITER_HOLDS_DOCUMENT(&iter)) {
+		v = bson_iter_value(&iter);
+		bson_init_static(&child, v->value.v_doc.data, v->value.v_doc.data_len);
+	}
+	bson_append_document_end(query, &child);
+#endif
+
+	if (bson_iter_init_find(&iter, raw_query, "projection") &&
+	    BSON_ITER_HOLDS_DOCUMENT(&iter)) {
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+		opts = bson_new();
+#endif
+		v = bson_iter_value(&iter);
+		bson_init_static(&proj, v->value.v_doc.data, v->value.v_doc.data_len);
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+		bson_append_document(opts, "projection", 10, &proj);
+#else
+		fields = &proj;
+#endif
+	}
+
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find_with_opts(col, query, opts, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw find",
+	                  NULL, 0, 0);
+
+	while (mongoc_cursor_next(cursor, &doc)) {
+#else
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find(col, MONGOC_QUERY_NONE,
+	                                0, 0, 0, query, fields, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw find",
+	                  NULL, 0, 0);
+
+	while (mongoc_cursor_more(cursor) && mongoc_cursor_next(cursor, &doc)) {
+#endif
+
+		*reply = pkg_realloc(*reply, (csz + 1) * sizeof **reply);
+		if (!*reply) {
+			LM_ERR("no more pkg\n");
+			ret = -1;
+			goto out_err;
+		}
+		(*reply)[csz] = pkg_malloc(expected_kv_no * sizeof ***reply);
+		if (!(*reply)[csz]) {
+			LM_ERR("no more pkg\n");
+			ret = -1;
+			goto out_err_free;
+		}
+
+		bson_iter_init(&iter, doc);
+		obj = json_object_new_object();
+		bson_to_json_generic(obj, &iter, BSON_TYPE_DOCUMENT);
+
+		p = json_object_to_json_string(obj);
+		if (!p) {
+			LM_ERR("failed to translate json to string\n");
+			ret = -1;
+			goto out_err_free;
+		}
+
+		LM_DBG("got JSON: %s\n", p);
+
+		len = strlen(p);
+		(*reply)[csz][0].val.s.s = pkg_malloc(len);
+		if (!(*reply)[csz][0].val.s.s ) {
+			LM_ERR("No more pkg \n");
+			ret = -1;
+			goto out_err_free;
+		}
+
+		memcpy((*reply)[csz][0].val.s.s, p, len);
+		(*reply)[csz][0].val.s.len = len;
+		(*reply)[csz][0].type = CDB_STR;
+
+		json_object_put(obj);
+
+		csz++;
+	}
+
+	*reply_no = csz;
+	if (opts)
+		bson_destroy(opts);
+	if (query != &_query)
+		bson_destroy(query);
+	mongoc_cursor_destroy(cursor);
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err_free:
+	if (*reply) {
+		for (i = 0; i < csz; i++) {
+			pkg_free((*reply)[i][0].val.s.s);
+			pkg_free((*reply)[i]);
+		}
+
+		pkg_free(*reply);
+	}
+out_err:
+	*reply = NULL;
+	*reply_no = 0;
+	if (opts)
+		bson_destroy(opts);
+	if (query != &_query)
+		bson_destroy(query);
+	mongoc_cursor_destroy(cursor);
+	mongoc_collection_destroy(col);
+	return ret;
+}
+
+int mongo_raw_update(cachedb_con *con, bson_t *raw_query, bson_iter_t *ns)
+{
+	mongoc_collection_t *col = NULL;
+	mongoc_bulk_operation_t *bulk = NULL;
+	bson_iter_t iter, uiter, sub_iter;
+	bson_error_t error;
+	bson_t query, update, reply;
+	struct timeval start;
+	const bson_value_t *v;
+	int ret, count = 0;
+	char *retstr;
+
+	if (bson_iter_type(ns) != BSON_TYPE_UTF8) {
+		LM_ERR("collection name must be a string (%d)!\n", bson_iter_type(ns));
+		return -1;
+	}
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   bson_iter_utf8(ns, NULL));
+
+	if (!bson_iter_init_find(&iter, raw_query, "updates") ||
+	    !BSON_ITER_HOLDS_ARRAY(&iter)) {
+		LM_ERR("missing or non-array 'updates' field in update command!\n");
+		return -1;
+	}
+
+	if (bson_iter_recurse(&iter, &sub_iter)) {
+		while (bson_iter_next(&sub_iter)) {
+			count++;
+		}
+	}
+
+	if (count == 0) {
+		LM_DBG("nothing to update!\n");
+		goto out;
+	}
+
+	bulk = mongoc_collection_create_bulk_operation(col, false, NULL);
+	if (!bulk) {
+		LM_ERR("failed to create bulk op!\n");
+		goto out_err;
+	}
+
+	count = 0;
+	if (bson_iter_init_find(&iter, raw_query, "updates") &&
+	    bson_iter_recurse(&iter, &uiter)) {
+		while (bson_iter_next(&uiter)) {
+			bson_iter_recurse(&uiter, &sub_iter);
+			if (!bson_iter_find(&sub_iter, "q")) {
+				LM_ERR("ignoring 'updates' subdoc due to missing q field!\n");
+				continue;
+			}
+			v = bson_iter_value(&sub_iter);
+			bson_init_static(&query, v->value.v_doc.data, v->value.v_doc.data_len);
+
+			bson_iter_recurse(&uiter, &sub_iter);
+			if (!bson_iter_find(&sub_iter, "u")) {
+				LM_ERR("ignoring 'updates' subdoc due to missing u field!\n");
+				continue;
+			}
+			v = bson_iter_value(&sub_iter);
+			bson_init_static(&update, v->value.v_doc.data, v->value.v_doc.data_len);
+
+			count++;
+			mongoc_bulk_operation_update(bulk, &query, &update, true);
+		}
+	}
+
+	if (count == 0) {
+		LM_DBG("nothing to update!\n");
+		goto out;
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+	ret = mongoc_bulk_operation_execute(bulk, &reply, &error);
+	if (!ret) {
+		LM_ERR("failed bulk update\nerror: %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw update",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw update",
+	                  NULL, 0, 0);
+
+	if (is_printable(L_DBG)) {
+		retstr = bson_as_json(&reply, NULL);
+		LM_DBG("reply received: %s\n", retstr);
+		bson_free(retstr);
+	}
+
+out:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return -1;
+}
+
+int mongo_raw_insert(cachedb_con *con, bson_t *raw_query, bson_iter_t *ns)
+{
+	mongoc_collection_t *col = NULL;
+	mongoc_bulk_operation_t *bulk = NULL;
+	bson_iter_t iter, sub_iter;
+	bson_error_t error;
+	bson_t doc, reply;
+	struct timeval start;
+	const bson_value_t *v;
+	int ret, count = 0;
+	char *retstr;
+
+	if (bson_iter_type(ns) != BSON_TYPE_UTF8) {
+		LM_ERR("collection name must be a string (%d)!\n", bson_iter_type(ns));
+		return -1;
+	}
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   bson_iter_utf8(ns, NULL));
+
+	if (!bson_iter_init_find(&iter, raw_query, "documents") ||
+	    !BSON_ITER_HOLDS_ARRAY(&iter)) {
+		LM_ERR("missing or non-array 'documents' field in raw insert!\n");
+		return -1;
+	}
+
+	if (bson_iter_recurse(&iter, &sub_iter)) {
+		while (bson_iter_next(&sub_iter)) {
+			count++;
+		}
+	}
+
+	if (count == 0) {
+		LM_DBG("nothing to insert!\n");
+		goto out;
+	}
+
+	bulk = mongoc_collection_create_bulk_operation(col, false, NULL);
+	if (!bulk) {
+		LM_ERR("failed to create bulk op!\n");
+		goto out_err;
+	}
+
+	if (bson_iter_init_find(&iter, raw_query, "documents") &&
+	    bson_iter_recurse(&iter, &sub_iter)) {
+		while (bson_iter_next(&sub_iter)) {
+			v = bson_iter_value(&sub_iter);
+			bson_init_static(&doc, v->value.v_doc.data, v->value.v_doc.data_len);
+			mongoc_bulk_operation_insert(bulk, &doc);
+		}
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+	ret = mongoc_bulk_operation_execute(bulk, &reply, &error);
+	if (!ret) {
+		LM_ERR("failed bulk insert\nerror: %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw insert",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw insert",
+	                  NULL, 0, 0);
+
+	if (is_printable(L_DBG)) {
+		retstr = bson_as_json(&reply, NULL);
+		LM_DBG("reply received: %s\n", retstr);
+		bson_free(retstr);
+	}
+
+out:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return -1;
+}
+
+int mongo_raw_remove(cachedb_con *con, bson_t *raw_query, bson_iter_t *ns)
+{
+	mongoc_collection_t *col = NULL;
+	mongoc_bulk_operation_t *bulk = NULL;
+	bson_iter_t iter, qiter, sub_iter;
+	bson_error_t error;
+	bson_t doc, reply;
+	struct timeval start;
+	const bson_value_t *v;
+	int ret, count = 0;
+	char *retstr;
+
+	if (bson_iter_type(ns) != BSON_TYPE_UTF8) {
+		LM_ERR("collection name must be a string (%d)!\n", bson_iter_type(ns));
+		return -1;
+	}
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   bson_iter_utf8(ns, NULL));
+
+	if (!bson_iter_init_find(&iter, raw_query, "deletes") ||
+	    !BSON_ITER_HOLDS_ARRAY(&iter)) {
+		LM_ERR("missing or non-array 'deletes' field in delete command!\n");
+		return -1;
+	}
+
+	if (bson_iter_recurse(&iter, &sub_iter)) {
+		while (bson_iter_next(&sub_iter)) {
+			count++;
+		}
+	}
+
+	if (count == 0) {
+		LM_DBG("nothing to delete!\n");
+		goto out;
+	}
+
+	bulk = mongoc_collection_create_bulk_operation(col, false, NULL);
+	if (!bulk) {
+		LM_ERR("failed to create bulk op!\n");
+		goto out_err;
+	}
+
+	count = 0;
+	if (bson_iter_init_find(&iter, raw_query, "deletes") &&
+	    bson_iter_recurse(&iter, &qiter)) {
+		while (bson_iter_next(&qiter)) {
+			bson_iter_recurse(&qiter, &sub_iter);
+			if (!bson_iter_find(&sub_iter, "q")) {
+				LM_ERR("ignoring 'deletes' subdoc due to missing q field!\n");
+				continue;
+			}
+			v = bson_iter_value(&sub_iter);
+			bson_init_static(&doc, v->value.v_doc.data, v->value.v_doc.data_len);
+
+			count++;
+			mongoc_bulk_operation_remove(bulk, &doc);
+		}
+	}
+
+	if (count == 0) {
+		LM_DBG("nothing to update!\n");
+		goto out;
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+	ret = mongoc_bulk_operation_execute(bulk, &reply, &error);
+	if (!ret) {
+		LM_ERR("failed bulk insert\nerror: %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "mongodb raw remove",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw remove",
+	                  NULL, 0, 0);
+
+	if (is_printable(L_DBG)) {
+		retstr = bson_as_json(&reply, NULL);
+		LM_DBG("reply received: %s\n", retstr);
+		bson_free(retstr);
+	}
+
+out:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (bulk) {
+		mongoc_bulk_operation_destroy(bulk);
+	}
+	mongoc_collection_destroy(col);
+	return -1;
+}
+
+static char *raw_query_buf;
+static int raw_query_buf_len;
+
+int mongo_con_raw_query(cachedb_con *con, str *qstr, cdb_raw_entry ***reply,
+                        int expected_kv_no, int *reply_no)
+{
+	struct json_object *obj = NULL;
+	bson_t doc, rpl;
+	bson_iter_t iter;
+	bson_error_t error;
+	struct timeval start;
+	int ret = 0;
+	const char *p;
+	int csz = 0, i, len;
+
+	LM_DBG("Get operation on namespace %s\n", MONGO_NAMESPACE(con));
+	start_expire_timer(start,mongo_exec_threshold);
+
+	if (qstr->len > raw_query_buf_len) {
+		raw_query_buf = pkg_realloc(raw_query_buf, qstr->len + 1);
+		if (!raw_query_buf) {
+			LM_ERR("oom!\n");
+			return -1;
+		}
+
+		memcpy(raw_query_buf, qstr->s, qstr->len);
+		raw_query_buf[qstr->len] = '\0';
+
+		raw_query_buf_len = qstr->len;
+	} else {
+		memcpy(raw_query_buf, qstr->s, qstr->len);
+		raw_query_buf[qstr->len] = '\0';
+	}
+
+	ret = json_to_bson(raw_query_buf, &doc);
+	if (ret < 0) {
+		LM_ERR("Failed to convert [%.*s] to BSON\n", qstr->len, qstr->s);
+		ret = -1;
+		goto out;
+	}
+
+	/* treat "find" differently on pre-3.2 MongoDB servers */
+	if ((compat_mode_30 || compat_mode_24) &&
+	    bson_iter_init_find(&iter, &doc, "find")) {
+		if (mongo_raw_find(con, &doc, &iter, reply, expected_kv_no, reply_no) != 0)
+			return -1;
+
+		if (*reply_no == 0)
+			return -2;
+
+		return 0;
+	} else if (compat_mode_24) {
+		if (bson_iter_init_find(&iter, &doc, "insert"))
+			return mongo_raw_insert(con, &doc, &iter);
+		else if (bson_iter_init_find(&iter, &doc, "update"))
+			return mongo_raw_update(con, &doc, &iter);
+		else if (bson_iter_init_find(&iter, &doc, "delete"))
+			return mongo_raw_remove(con, &doc, &iter);
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_command_simple(MONGO_COLLECTION(con), &doc,
+	                              NULL, &rpl, &error)) {
+		LM_ERR("raw query:\n'%.*s'\nfailed with: %d.%d: %s\n", qstr->len, qstr->s,
+		       error.domain, error.code, error.message);
+		ret = -1;
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw query",
+		                  qstr->s, qstr->len, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB raw query",
+	                  qstr->s, qstr->len, 0);
+
+	/* start with a single returned document */
+	*reply = pkg_malloc(1 * sizeof **reply);
+	if (!*reply) {
+		LM_ERR("no more PKG mem\n");
+		return -1;
+	}
+
+	/* expected_kv_no is always 1 for MongoDB */
+	**reply = pkg_malloc(expected_kv_no * sizeof ***reply);
+	if (!**reply) {
+		LM_ERR("No more pkg mem\n");
+		pkg_free(*reply);
+		return -1;
+	}
+
+	if (!bson_iter_init(&iter, &rpl)) {
+		LM_ERR("failed to init!!!\n");
+		ret = -1;
+		goto out_err;
+	}
+
+	do {
+		if (csz > 0) {
+			*reply = pkg_realloc(*reply, (csz + 1) * sizeof **reply);
+			if (!*reply) {
+				LM_ERR("No more pkg\n");
+				ret = -1;
+				goto out_err;
+			}
+			(*reply)[csz] = pkg_malloc(expected_kv_no * sizeof ***reply);
+			if (!(*reply)[csz]) {
+				LM_ERR("No more pkg\n");
+				ret = -1;
+				goto out_err;
+			}
+		}
+
+		obj = json_object_new_object();
+		bson_to_json_generic(obj, &iter, BSON_TYPE_DOCUMENT);
+
+		p = json_object_to_json_string(obj);
+		if (!p) {
+			LM_ERR("failed to translate json to string\n");
+			ret = -1;
+			goto out_err;
+		}
+
+		LM_DBG("got JSON: %s\n", p);
+
+		len = strlen(p);
+		(*reply)[csz][0].val.s.s = pkg_malloc(len);
+		if (!(*reply)[csz][0].val.s.s ) {
+			LM_ERR("No more pkg \n");
+			ret = -1;
+			goto out_err;
+		}
+
+		memcpy((*reply)[csz][0].val.s.s,p,len);
+		(*reply)[csz][0].val.s.len = len;
+		(*reply)[csz][0].type = CDB_STR;
+
+		json_object_put(obj);
+
+		csz++;
+	} while (bson_iter_next(&iter));
+
+out:
+	*reply_no = csz;
+	if (csz == 0)
+		return -2;
+
+	return 1;
+
+out_err:
+	if (obj)
+		json_object_put(obj);
+
+	if (*reply) {
+		for (i = 0; i < csz; i++) {
+			pkg_free((*reply)[i][0].val.s.s);
+			pkg_free((*reply)[i]);
+		}
+
+		pkg_free(*reply);
+		*reply = NULL;
+	}
+
+	return ret;
+}
+
+int mongo_con_add(cachedb_con *con, str *attr, int val, int expires, int *new_val)
+{
+	bson_t *cmd;
+	bson_t child, ichild, reply;
+	bson_error_t error;
+	bson_iter_t iter;
+	bson_iter_t sub_iter;
+	struct timeval start;
+	int ret = 0;
+
+	cmd = bson_new();
+	bson_append_utf8(cmd, "findAndModify", 13,
+	                 mongoc_collection_get_name(MONGO_COLLECTION(con)), -1);
+
+	BSON_APPEND_DOCUMENT_BEGIN(cmd, "query", &child);
+	bson_append_utf8(&child, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+	bson_append_document_end(cmd, &child);
+
+	BSON_APPEND_DOCUMENT_BEGIN(cmd, "update", &child);
+	BSON_APPEND_DOCUMENT_BEGIN(&child, "$inc", &ichild);
+	bson_append_int32(&ichild, "opensips_counter", 16, val);
+	bson_append_document_end(&child, &ichild);
+	bson_append_document_end(cmd, &child);
+
+	bson_append_bool(cmd, "upsert", 6, true);
+
+	bson_append_bool(cmd, "new", 3, true);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_command_simple(MONGO_COLLECTION(con), cmd,
+	                              NULL, &reply, &error)) {
+		LM_ERR("failed to %s: %.*s += %d\n", val > 0 ? "add" : "sub",
+		       attr->len, attr->s, val);
+		ret = -1;
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB counter add",
+		                  NULL, 0, 0);
+		goto out;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB counter add",
+	                  NULL, 0, 0);
+
+	if (!new_val)
+		goto out;
+
+	if (bson_iter_init_find(&iter, &reply, "value") &&
+	    BSON_ITER_HOLDS_DOCUMENT(&iter) &&
+	    bson_iter_recurse(&iter, &sub_iter)) {
+
+		if (bson_iter_find(&sub_iter, "opensips_counter")) {
+			*new_val = bson_iter_value(&sub_iter)->value.v_int32;
+		}
+	}
+
+out:
+	bson_destroy(cmd);
+	return ret;
+}
+
+int mongo_con_sub(cachedb_con *connection,str *attr,int val,int expires,int *new_val)
+{
+	return mongo_con_add(connection,attr,-val,expires,new_val);
+}
+
+int mongo_con_get_counter(cachedb_con *con, str *attr, int *val)
+{
+	bson_t *query;
+	const bson_t *doc;
+	const bson_value_t *value;
+	mongoc_cursor_t *cursor;
+	bson_iter_t iter;
+	struct timeval start;
+	int ret = 0;
+
+	query = bson_new();
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	bson_append_utf8(query, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+#else
+	bson_t child;
+	BSON_APPEND_DOCUMENT_BEGIN(query, "$query", &child);
+	bson_append_utf8(&child, MDB_PK, MDB_PKLEN, attr->s, attr->len);
+	bson_append_document_end(query, &child);
+#endif
+
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find_with_opts(
+	                MONGO_COLLECTION(con), query, NULL, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB counter get",
+	                  NULL, 0, 0);
+
+	while (mongoc_cursor_next(cursor, &doc)) {
+#else
+	start_expire_timer(start, mongo_exec_threshold);
+	cursor = mongoc_collection_find(MONGO_COLLECTION(con), MONGOC_QUERY_NONE,
+	                                0, 0, 0, query, NULL, NULL);
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB counter get",
+	                  NULL, 0, 0);
+
+	while (mongoc_cursor_more(cursor) && mongoc_cursor_next(cursor, &doc)) {
+#endif
+
+		if (bson_iter_init_find(&iter, doc, "opensips_counter")) {
+			value = bson_iter_value(&iter);
+			switch (value->value_type) {
+			case BSON_TYPE_INT32:
+				*val = value->value.v_int32;
+				break;
+			default:
+				LM_ERR("unsupported type %d for key %.*s!\n", attr->len,
+				       value->value_type, attr->s);
+				ret = -1;
+				goto out;
+			}
+		}
+	}
+
+out:
+	bson_destroy(query);
+	mongoc_cursor_destroy(cursor);
+	return ret;
+}
+
+int kvo_to_bson(const db_key_t *_k, const db_val_t *_v, const db_op_t *_op,
+                 int _n, bson_t *doc)
+{
+	int i;
+	bool ok = true;
+	bson_t _child, *child;
+	str key;
+	bson_oid_t _id;
+	bool has_oid = false;
+	char *p, _old_char;
+
+	for (i = 0; i < _n; i++) {
+		if (!_op || strcmp(_op[i], OP_EQ) == 0) {
+			child = doc;
+			key = *_k[i];
+		} else {
+			child = &_child;
+			bson_append_document_begin(doc, _k[i]->s, _k[i]->len, &_child);
+			if (strcmp(_op[i], OP_LT) == 0) {
+				key.s = "$lt";
+				key.len = 3;
+			} else if (strcmp(_op[i], OP_GT) == 0) {
+				key.s = "$gt";
+				key.len = 3;
+			} else if (strcmp(_op[i], OP_LEQ) == 0) {
+				key.s = "$lte";
+				key.len = 4;
+			} else if (strcmp(_op[i], OP_GEQ) == 0) {
+				key.s = "$gte";
+				key.len = 4;
+			} else if (strcmp(_op[i], OP_NEQ) == 0) {
+				key.s = "$ne";
+				key.len = 3;
+			}
+		}
+
+		if (VAL_NULL(&_v[i])) {
+			if (!bson_append_null(child, key.s, key.len)) {
+				LM_ERR("NULL NOT SUPPORTED X\n");
+				return -1;
+			}
+			continue;
+		}
+
+		switch (VAL_TYPE(&_v[i])) {
+			case DB_INT:
+				ok = bson_append_int32(child, key.s, key.len, VAL_INT(&_v[i]));
+				break;
+			case DB_STRING:
+				if (!has_oid && _k[i]->len == 3 &&
+				    strncmp("_id", _k[i]->s, _k[i]->len) == 0) {
+					LM_DBG("we got it [%.*s]\n", _k[i]->len, _k[i]->s);
+					bson_oid_init_from_string(&_id, VAL_STRING(&_v[i]));
+					ok = bson_append_oid(child, key.s, key.len, &_id);
+					has_oid = true;
+				} else {
+					ok = bson_append_utf8(child, key.s, key.len, VAL_STRING(&_v[i]), -1);
+				}
+				break;
+			case DB_STR:
+				if (!has_oid && _k[i]->len == 3 &&
+				    strncmp("_id", _k[i]->s, _k[i]->len) == 0) {
+					p = VAL_STR(&_v[i]).s + VAL_STR(&_v[i]).len;
+					_old_char = *p;
+					*p = '\0';
+					bson_oid_init_from_string(&_id, VAL_STR(&_v[i]).s);
+					*p = _old_char;
+					ok = bson_append_oid(child, key.s, key.len, &_id);
+					has_oid = true;
+				} else {
+					ok = bson_append_utf8(child, key.s, key.len, VAL_STR(&_v[i]).s,
+						VAL_STR(&_v[i]).len);
+				}
+				break;
+			case DB_BLOB:
+				ok = bson_append_utf8(child, key.s, key.len, VAL_BLOB(&_v[i]).s,
+						VAL_BLOB(&_v[i]).len);
+				break;
+			case DB_DOUBLE:
+				ok = bson_append_double(child, key.s, key.len, VAL_DOUBLE(&_v[i]));
+				break;
+			case DB_BIGINT:
+				ok = bson_append_int64(child, key.s, key.len, VAL_BIGINT(&_v[i]));
+				break;
+			case DB_DATETIME:
+				ok = bson_append_time_t(child, key.s, key.len, VAL_TIME(&_v[i]));
+				break;
+			case DB_BITMAP:
+				ok = bson_append_int32(child, key.s, key.len, VAL_BITMAP(&_v[i]));
+				break;
+		}
+
+		if (!ok) {
+			LM_ERR("failed to append bson for key=%.*s, op=%s\n",
+			       _k[i]->len, _k[i]->s, _op ? _op[i] : NULL);
+			return -1;
+		}
+
+		if (_op && strcmp(_op[i], OP_EQ) != 0) {
+			if (!bson_append_document_end(doc, child)) {
+				LM_ERR("failed to append doc end!\n");
+				return -1;
+			}
+		}
+	}
+
+	return 0;
+}
+
+int mongo_db_query_trans(cachedb_con *con, const str *table, const db_key_t *_k,
+                         const db_op_t *_op, const db_val_t *_v,
+                         const db_key_t *_c, const int _n, const int _nc,
+                         const db_key_t _o, db_res_t **_r)
+{
+	char key_buff[32], namespace[MDB_MAX_NS_LEN], *p;
+	char hex_oid[HEX_OID_SIZE];
+	static str dummy_string = {"", 0};
+	bson_t *filter, child;
+	mongoc_cursor_t *cursor = NULL;
+	const bson_t *doc;
+	db_row_t *current;
+	db_val_t *cur_val;
+	bson_iter_t iter;
+	struct timeval start;
+	int ri, c, old_rows=0, rows = 0;
+	mongoc_collection_t *col = NULL;
+	char *strf, *stro;
+
+	*_r = NULL;
+
+	filter = bson_new();
+
+#if !MONGOC_CHECK_VERSION(1, 5, 0)
+	bson_t *fields = NULL;
+	BSON_APPEND_DOCUMENT_BEGIN(filter, "$query", &child);
+	if (kvo_to_bson(_k, _v, _op, _n, &child) != 0) {
+		LM_ERR("failed to build filter bson\n");
+		goto out_err;
+	}
+	bson_append_document_end(filter, &child);
+#else
+	bson_t *opts = NULL;
+	if (kvo_to_bson(_k, _v, _op, _n, filter) != 0) {
+		LM_ERR("failed to build filter bson\n");
+		goto out_err;
+	}
+#endif
+
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	opts = bson_new();
+	if (_o) {
+		bson_append_document_begin(opts, "sort", 4, &child);
+		bson_append_int32(&child, _o->s, _o->len, 1);
+		bson_append_document_end(opts, &child);
+	}
+
+	bson_append_document_begin(opts, "projection", 10, &child);
+	for (c = 0; c < _nc; c++) {
+		bson_append_int32(&child, _c[c]->s, _c[c]->len, 1);
+	}
+	bson_append_document_end(opts, &child);
+#else
+	if (_o) {
+		BSON_APPEND_DOCUMENT_BEGIN(filter, "$orderby", &child);
+		bson_append_int32(&child, _o->s, _o->len, 1);
+		bson_append_document_end(filter, &child);
+	}
+
+	fields = bson_new();
+	for (c = 0; c < _nc; c++) {
+		bson_append_int32(fields, _c[c]->s, _c[c]->len, 1);
+	}
+#endif
+
+	memcpy(namespace, table->s, table->len);
+	namespace[table->len] = '\0';
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   namespace);
+
+	if (is_printable(L_DBG)) {
+		strf = bson_as_json(filter, NULL);
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+		stro = bson_as_json(opts, NULL);
+#else
+		stro = bson_as_json(fields, NULL);
+#endif
+		LM_DBG("query doc:\n%s\n%s\n", strf, stro);
+		bson_free(strf);
+		bson_free(stro);
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	cursor = mongoc_collection_find_with_opts(col, filter, opts, NULL);
+#else
+	cursor = mongoc_collection_find(col, MONGOC_QUERY_NONE,
+	                                0, 0, 0, filter, fields, NULL);
+#endif
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB query trans",
+	                  NULL, 0, 0);
+
+	MONGO_CURSOR(con) = cursor;
+
+	*_r = db_new_result();
+	if (!*_r) {
+		LM_ERR("Failed to init new result \n");
+		goto out_err;
+	}
+
+	RES_COL_N(*_r) = _nc;
+
+	/* on first iteration we allocate the result
+	 * we always assume the query returns exactly the number
+	 * of 'columns' as were requested */
+	if (db_allocate_columns(*_r, _nc) != 0) {
+		LM_ERR("failed to allocate columns\n");
+		goto out_err;
+	}
+
+	/* and we initialize the names as if all are there */
+	for (c = 0; c < _nc; c++) {
+		/* since we don't have schema, the types will be allocated
+		 * when we fetch the actual rows */
+		RES_NAMES(*_r)[c]->s = _c[c]->s;
+		RES_NAMES(*_r)[c]->len = _c[c]->len;
+	}
+
+
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	for (ri = 0; mongoc_cursor_next(cursor, &doc); ri++) {
+#else
+	for (ri = 0; mongoc_cursor_more(cursor) &&
+	             mongoc_cursor_next(cursor, &doc); ri++) {
+#endif
+		if ((ri + 1) > rows) {
+			old_rows = rows;
+			rows = rows > 0 ? 2 * rows : 1;
+			if(old_rows) {
+				if (db_realloc_rows(*_r, old_rows, rows) != 0) {
+					LM_ERR("failed to realloc rows\n");
+					goto out_err;
+				}
+
+				hex_oid_id = pkg_realloc(hex_oid_id,
+				                         sizeof *hex_oid_id * rows * HEX_OID_SIZE);
+				if (!hex_oid_id) {
+					LM_ERR("oom\n");
+					goto out_err;
+				}
+			} else {
+				if (db_allocate_rows(*_r,rows) != 0) {
+					LM_ERR("No more private memory for rows \n");
+					goto out_err;
+				}
+
+				hex_oid_id = pkg_malloc(sizeof(char) * rows* HEX_OID_SIZE);
+				if (!hex_oid_id) {
+					LM_ERR("oom\n");
+					goto out_err;
+				}
+			}
+		}
+
+		RES_ROW_N(*_r) = ri + 1;
+
+		current = &(RES_ROWS(*_r)[ri]);
+		ROW_N(current) = RES_COL_N(*_r);
+		for (c = 0; c < _nc; c++) {
+			memcpy(key_buff, _c[c]->s, _c[c]->len);
+			key_buff[_c[c]->len] = '\0';
+			cur_val = &ROW_VALUES(current)[c];
+
+			if (!bson_iter_init_find(&iter, doc, key_buff)) {
+				memset(cur_val, 0, sizeof *cur_val);
+				VAL_STRING(cur_val) = dummy_string.s;
+				VAL_STR(cur_val) = dummy_string;
+				VAL_BLOB(cur_val) = dummy_string;
+				/* we treat null values as DB string */
+				VAL_TYPE(cur_val) = DB_STRING;
+				VAL_NULL(cur_val) = 1;
+				LM_DBG("fixed missing col: '%.*s'\n", _c[c]->len, _c[c]->s);
+			} else {
+				switch (bson_iter_type(&iter)) {
+					case BSON_TYPE_INT32:
+						VAL_TYPE(cur_val) = DB_INT;
+						VAL_INT(cur_val) = bson_iter_int32(&iter);
+						LM_DBG("Found int [%.*s]=[%d]\n",
+						       _c[c]->len, _c[c]->s, VAL_INT(cur_val));
+						break;
+					case BSON_TYPE_DOUBLE:
+						VAL_TYPE(cur_val) = DB_DOUBLE;
+						VAL_DOUBLE(cur_val) = bson_iter_double(&iter);
+						LM_DBG("Found double [%.*s]=[%f]\n",
+						       _c[c]->len, _c[c]->s, VAL_DOUBLE(cur_val));
+						break;
+					case BSON_TYPE_UTF8:
+						VAL_TYPE(cur_val) = DB_STRING;
+						VAL_STRING(cur_val) = bson_iter_utf8(&iter, NULL);
+						LM_DBG("Found string [%.*s]=[%s]\n",
+						       _c[c]->len, _c[c]->s, VAL_STRING(cur_val));
+						break;
+					case BSON_TYPE_INT64:
+						VAL_TYPE(cur_val) = DB_BIGINT;
+						VAL_BIGINT(cur_val) = bson_iter_int64(&iter);
+						LM_DBG("Found long [%.*s]=[%lld]\n",
+						       _c[c]->len, _c[c]->s, VAL_BIGINT(cur_val));
+						break;
+					case BSON_TYPE_DATE_TIME:
+						VAL_TYPE(cur_val) = DB_DATETIME;
+						VAL_TIME(cur_val) = bson_iter_date_time(&iter)/(int64_t)1000;
+						LM_DBG("Found time [%.*s]=[%d]\n",
+						       _c[c]->len, _c[c]->s, (int)VAL_TIME(cur_val));
+						break;
+					case BSON_TYPE_OID:
+						bson_oid_to_string(bson_iter_oid(&iter), hex_oid);
+						p = &hex_oid_id[ri * HEX_OID_SIZE];
+						memcpy(p, hex_oid, HEX_OID_SIZE);
+						VAL_TYPE(cur_val) = DB_STRING;
+						VAL_STRING(cur_val) = p;
+						LM_DBG("Found oid [%.*s]=[%s]\n",
+						       _c[c]->len, _c[c]->s, VAL_STRING(cur_val));
+						break;
+					case BSON_TYPE_NULL:
+						VAL_TYPE(cur_val) = DB_STRING;
+						VAL_NULL(cur_val) = 1;
+						LM_DBG("Found null [%.*s]=[%d]\n",
+						       _c[c]->len, _c[c]->s, VAL_NULL(cur_val));
+						break;
+					default:
+						LM_WARN("Unsupported type [%d] for [%.*s] - treating as NULL\n",
+						        bson_iter_type(&iter), _c[c]->len, _c[c]->s);
+						memset(cur_val, 0, sizeof *cur_val);
+						VAL_STRING(cur_val) = dummy_string.s;
+						VAL_STR(cur_val) = dummy_string;
+						VAL_BLOB(cur_val) = dummy_string;
+						/* we treat null values as DB string */
+						VAL_TYPE(cur_val) = DB_STRING;
+						VAL_NULL(cur_val) = 1;
+						break;
+				}
+			}
+		}
+	}
+
+	bson_destroy(filter);
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	bson_destroy(opts);
+#else
+	bson_destroy(fields);
+#endif
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	bson_destroy(filter);
+#if MONGOC_CHECK_VERSION(1, 5, 0)
+	if (opts) {
+		bson_destroy(opts);
+	}
+#else
+	if (fields) {
+		bson_destroy(fields);
+	}
+#endif
+	if (cursor) {
+		mongoc_cursor_destroy(cursor);
+	}
+	if (*_r) {
+		db_free_result(*_r);
+		*_r = NULL;
+
+		if (hex_oid_id) {
+			pkg_free(hex_oid_id);
+			hex_oid_id = NULL;
+		}
+	}
+	if (col) {
+		mongoc_collection_destroy(col);
+	}
+	return -1;
+}
+
+int mongo_db_free_result_trans(cachedb_con *con, db_res_t *_r)
+{
+	if (!con || !_r) {
+		LM_ERR("invalid parameter value\n");
+		return -1;
+	}
+
+	LM_DBG("freeing mongo query result \n");
+
+	if (hex_oid_id) {
+		pkg_free(hex_oid_id);
+		hex_oid_id = NULL;
+	}
+
+	if (db_free_result(_r) < 0) {
+		LM_ERR("unable to free result structure\n");
+		return -1;
+	}
+
+	mongoc_cursor_destroy(MONGO_CURSOR(con));
+	MONGO_CURSOR(con) = NULL;
+	return 0;
+}
+
+int mongo_db_insert_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_val_t *_v, const int _n)
+{
+	char namespace[MDB_MAX_NS_LEN];
+	bson_t *doc;
+	bson_error_t error;
+	mongoc_collection_t *col = NULL;
+	struct timeval start;
+	char *retstr;
+
+	doc = bson_new();
+	if (kvo_to_bson(_k, _v, NULL, _n, doc) != 0) {
+		LM_ERR("failed to build bson\n");
+		goto out_err;
+	}
+
+	if (is_printable(L_DBG)) {
+		retstr = bson_as_json(doc, NULL);
+		LM_DBG("insert doc:\n%s\n", retstr);
+		bson_free(retstr);
+	}
+
+	memcpy(namespace, table->s, table->len);
+	namespace[table->len] = '\0';
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   namespace);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_insert(col, MONGOC_INSERT_NONE, doc, NULL, &error)) {
+	    LM_ERR("insert failed with:\nerror %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB insert trans",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB insert trans",
+	                  NULL, 0, 0);
+
+	if (doc) {
+		bson_destroy(doc);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (doc) {
+		bson_destroy(doc);
+	}
+	if (col) mongoc_collection_destroy(col);
+	return -1;
+}
+
+int mongo_db_delete_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_op_t *_o,
+                          const db_val_t *_v, const int _n)
+{
+	char namespace[MDB_MAX_NS_LEN];
+	bson_t *doc;
+	bson_error_t error;
+	mongoc_collection_t *col = NULL;
+	struct timeval start;
+	char *retstr;
+
+	doc = bson_new();
+	if (kvo_to_bson(_k, _v, _o, _n, doc) != 0) {
+		LM_ERR("failed to build bson\n");
+		goto out_err;
+	}
+
+	memcpy(namespace, table->s, table->len);
+	namespace[table->len] = '\0';
+
+	if (is_printable(L_DBG)) {
+		retstr = bson_as_json(doc, NULL);
+		LM_DBG("remove doc:\n%s\n", retstr);
+		bson_free(retstr);
+	}
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   namespace);
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_remove(col, MONGOC_REMOVE_NONE, doc, NULL, &error)) {
+	    LM_ERR("insert failed with:\nerror %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB remove trans",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB remove trans",
+	                  NULL, 0, 0);
+
+	if (doc) {
+		bson_destroy(doc);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (doc) {
+		bson_destroy(doc);
+	}
+	if (col) mongoc_collection_destroy(col);
+	return -1;
+}
+
+int mongo_db_update_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_op_t *_o,
+                          const db_val_t *_v, const db_key_t *_uk,
+                          const db_val_t *_uv, const int _n, const int _un)
+{
+	char namespace[MDB_MAX_NS_LEN];
+	bson_t *query, *update = NULL, child;
+	bson_error_t error;
+	mongoc_collection_t *col = NULL;
+	struct timeval start;
+	char *strq, *stru;
+
+	query = bson_new();
+	if (kvo_to_bson(_k, _v, _o, _n, query) != 0) {
+		LM_ERR("failed to build query bson\n");
+		goto out_err;
+	}
+
+	update = bson_new();
+	BSON_APPEND_DOCUMENT_BEGIN(update, "$set", &child);
+	if (kvo_to_bson(_uk, _uv, NULL, _un, &child) != 0) {
+		LM_ERR("failed to build update bson\n");
+		goto out_err;
+	}
+	bson_append_document_end(update, &child);
+
+	memcpy(namespace, table->s, table->len);
+	namespace[table->len] = '\0';
+
+	col = mongoc_client_get_collection(MONGO_CLIENT(con), MONGO_DB_STR(con),
+	                                   namespace);
+
+	if (is_printable(L_DBG)) {
+		strq = bson_as_json(query, NULL);
+		stru = bson_as_json(update, NULL);
+		LM_DBG("update docs:\n%s\n%s\n", strq, stru);
+		bson_free(strq);
+		bson_free(stru);
+	}
+
+	start_expire_timer(start, mongo_exec_threshold);
+	if (!mongoc_collection_update(col, MONGOC_UPDATE_MULTI_UPDATE,
+	                              query, update, NULL, &error)) {
+	    LM_ERR("insert failed with:\nerror %d.%d: %s\n",
+		       error.domain, error.code, error.message);
+		stop_expire_timer(start, mongo_exec_threshold, "MongoDB update trans",
+		                  NULL, 0, 0);
+		goto out_err;
+	}
+	stop_expire_timer(start, mongo_exec_threshold, "MongoDB update trans",
+	                  NULL, 0, 0);
+
+	if (query) {
+		bson_destroy(query);
+	}
+	if (update) {
+		bson_destroy(update);
+	}
+	mongoc_collection_destroy(col);
+	return 0;
+
+out_err:
+	if (query) {
+		bson_destroy(query);
+	}
+	if (update) {
+		bson_destroy(update);
+	}
+	if (col) mongoc_collection_destroy(col);
+	return -1;
+}
diff --git a/modules/cachedb_mongodb2/cachedb_mongodb_dbase.h b/modules/cachedb_mongodb2/cachedb_mongodb_dbase.h
new file mode 100644
index 0000000..244ee8b
--- /dev/null
+++ b/modules/cachedb_mongodb2/cachedb_mongodb_dbase.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 2011-2017 OpenSIPS Project
+ *
+ * This file is part of opensips, a free SIP server.
+ *
+ * opensips is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * opensips is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+ */
+
+#ifndef CACHEDBMONGO_DBASE_H
+#define CACHEDBMONGO_DBASE_H
+
+#include "../../cachedb/cachedb.h"
+#include "../../db/db.h"
+
+#define MONGO_HAVE_STDINT 1
+
+#include <mongoc.h>
+#include <bson.h>
+
+#include <json.h>
+#include <stdint.h>
+
+extern int mongo_op_timeout;
+
+#define MDB_PK    "_id"
+#define MDB_PKLEN 3
+#define MDB_MAX_NS_LEN 120
+
+typedef struct {
+	struct cachedb_id *id;
+	unsigned int ref;
+	struct cachedb_pool_con_t *next;
+
+	/* shortcuts for raw queries */
+	char *db;
+	char *col;
+
+	/* only if we connect to a repl set*/
+	char *replset_name;
+
+	mongoc_client_t *client;
+	mongoc_collection_t *collection;
+	mongoc_database_t *database;
+
+	/* cursor result for the query */
+	mongoc_cursor_t *cursor;
+} mongo_con;
+
+#define MONGO_CON(mon_con)			((mon_con)->connection)
+#define MONGO_CLIENT(cdb_con)		(((mongo_con *)((cdb_con)->data))->client)
+#define MONGO_CURSOR(cdb_con)		(((mongo_con *)((cdb_con)->data))->cursor)
+#define MONGO_NAMESPACE(cdb_con)	(((mongo_con *)((cdb_con)->data))->id->database)
+#define MONGO_DB_STR(cdb_con)		(((mongo_con *)((cdb_con)->data))->db)
+#define MONGO_COL_STR(cdb_con)		(((mongo_con *)((cdb_con)->data))->col)
+#define MONGO_DATABASE(cdb_con)		(((mongo_con *)((cdb_con)->data))->database)
+#define MONGO_COLLECTION(cdb_con)	(((mongo_con *)((cdb_con)->data))->collection)
+
+cachedb_con* mongo_con_init(str *url);
+void mongo_con_destroy(cachedb_con *con);
+int mongo_con_get(cachedb_con *con,str *attr,str *val);
+int mongo_con_set(cachedb_con *con,str *attr,str *val,int expires);
+int mongo_con_remove(cachedb_con *connection,str *attr);
+int mongo_con_raw_query(cachedb_con *con, str *qstr, cdb_raw_entry ***reply,
+                        int expected_kv_no, int *reply_no);
+int mongo_con_add(cachedb_con *connection,str *attr,int val,int expires,int *new_val);
+int mongo_con_sub(cachedb_con *connection,str *attr,int val,int expires,int *new_val);
+int mongo_con_get_counter(cachedb_con *connection,str *attr,int *val);
+int mongo_db_query_trans(cachedb_con *con, const str *table, const db_key_t *_k,
+                         const db_op_t *_op, const db_val_t *_v,
+                         const db_key_t *_c, const int _n, const int _nc,
+                         const db_key_t _o, db_res_t **_r);
+int mongo_db_free_result_trans(cachedb_con* con, db_res_t* _r);
+int mongo_db_insert_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_val_t *_v, const int _n);
+int mongo_db_delete_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_op_t *_o,
+                          const db_val_t *_v, const int _n);
+int mongo_db_update_trans(cachedb_con *con, const str *table,
+                          const db_key_t *_k, const db_op_t *_o,
+                          const db_val_t *_v, const db_key_t *_uk,
+                          const db_val_t *_uv, const int _n, const int _un);
+#endif /* CACHEDBMONGO_DBASE_H */
+
diff --git a/modules/cachedb_mongodb2/cachedb_mongodb_json.c b/modules/cachedb_mongodb2/cachedb_mongodb_json.c
new file mode 100644
index 0000000..f60bf25
--- /dev/null
+++ b/modules/cachedb_mongodb2/cachedb_mongodb_json.c
@@ -0,0 +1,272 @@
+/*
+ * Copyright (C) 2011-2017 OpenSIPS Project
+ *
+ * This file is part of opensips, a free SIP server.
+ *
+ * opensips is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * opensips is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+ */
+
+#include "../../dprint.h"
+#include "../../ut.h"
+#include "cachedb_mongodb_json.h"
+#include "cachedb_mongodb_dbase.h"
+
+int json_to_bson_append_element(bson_t *doc, const char *k, struct json_object *v);
+
+int json_to_bson_append_array(bson_t *doc, struct json_object *a)
+{
+	int i, al_len;
+	char *al;
+	json_object *it;
+
+	for (i = 0; i < json_object_array_length(a); i++) {
+		al = int2str(i, &al_len);
+		if (!al) {
+			LM_ERR("Failed to convert %d to str\n", i);
+			return -1;
+		}
+
+		al[al_len] = '\0';
+		it = json_object_array_get_idx(a, i);
+		if (!it) {
+			LM_ERR("Failed to get JSON idx\n");
+			return -1;
+		}
+
+		if (json_to_bson_append_element(doc, al, it) < 0) {
+			LM_ERR("Failed to append element to BSON\n");
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+# define json_object_object_iterator(obj,key,val) \
+	char *key; struct json_object *val; struct lh_entry *entry; \
+	for(entry = json_object_get_object(obj)->head; \
+		(entry ? (key = (char*)entry->k, \
+		val = (struct json_object*)entry->v, entry) : 0); \
+		entry = entry->next)
+
+int json_to_bson_append(bson_t *doc, struct json_object *o)
+{
+	json_object_object_iterator(o, key, val) {
+		if (json_to_bson_append_element(doc, key, val) < 0) {
+			LM_ERR("Failed to append new element\n");
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+int json_to_bson_append_element(bson_t *doc, const char *k, struct json_object *v)
+{
+	bson_t child;
+
+	if (!v) {
+		bson_append_null(doc, k, -1);
+		return 0;
+	}
+
+	switch (json_object_get_type(v)) {
+		case json_type_int:
+			if (!bson_append_int32(doc, k, -1, json_object_get_int(v))) {
+				LM_ERR("Failed to append int\n");
+				return -1;
+			}
+			break;
+		case json_type_boolean:
+			if (!bson_append_bool(doc, k, -1, json_object_get_boolean(v))) {
+				LM_ERR("Failed to append boolean\n");
+				return -1;
+			}
+			break;
+		case json_type_double:
+			if (!bson_append_double(doc, k, -1, json_object_get_double(v))) {
+				LM_ERR("Failed to append double\n");
+				return -1;
+			}
+			break;
+		case json_type_string:
+			if (!bson_append_utf8(doc, k, -1, json_object_get_string(v), -1)) {
+				LM_ERR("Failed to append string\n");
+				return -1;
+			}
+			break;
+		case json_type_object:
+			BSON_APPEND_DOCUMENT_BEGIN(doc, k, &child);
+			if (json_to_bson_append(&child, v) < 0) {
+				LM_ERR("Failed to append to bson_t\n");
+				return -1;
+			}
+			bson_append_document_end(doc, &child);
+			break;
+		case json_type_array:
+			BSON_APPEND_ARRAY_BEGIN(doc, k, &child);
+			if (json_to_bson_append_array(&child, v) < 0) {
+				LM_ERR("Failed to append array to bson_t\n");
+				return -1;
+			}
+			bson_append_array_end(doc, &child);
+			break;
+		default:
+			LM_ERR("Can't handle type for : %s\n", json_object_to_json_string(v));
+			return -1;
+	}
+
+	return 0;
+}
+
+int json_to_bson(char *json, bson_t *doc)
+{
+	struct json_object *obj;
+
+	LM_DBG("Trying to convert [%s]\n", json);
+
+	obj = json_tokener_parse(json);
+	if (is_error(obj)) {
+		LM_ERR("Failed to parse JSON: %s\n", json);
+		return -2;
+	}
+
+	if (!json_object_is_type(obj, json_type_object)) {
+		LM_ERR("Inconsistent JSON type\n");
+		goto error;
+	}
+
+	bson_init(doc);
+	if (json_to_bson_append(doc, obj) < 0) {
+		LM_ERR("Failed to convert json to bson_t\n");
+		bson_destroy(doc);
+		goto error;
+	}
+
+	json_object_put(obj);
+	return 0;
+
+error:
+	if (obj)
+		json_object_put(obj);
+	return -1;
+}
+
+void bson_to_json_generic(struct json_object *obj, bson_iter_t *it,
+                          bson_type_t type)
+{
+	const char *curr_key;
+	char *s, oid[25];
+	int len;
+	struct json_object *obj2 = NULL;
+	bson_iter_t it2;
+
+	while (bson_iter_next(it)) {
+		curr_key = bson_iter_key(it);
+		switch (bson_iter_type(it) ) {
+				case BSON_TYPE_INT32:
+					LM_DBG("Found key %s with type int\n", curr_key);
+					if (type == BSON_TYPE_DOCUMENT) {
+						json_object_object_add(obj,curr_key,
+						           json_object_new_int(bson_iter_int32(it)));
+					} else if (type == BSON_TYPE_ARRAY) {
+						json_object_array_add(obj,
+						     json_object_new_int(bson_iter_int32(it)));
+					}
+					break;
+				case BSON_TYPE_INT64:
+					LM_DBG("Found key %s with type long\n", curr_key);
+					/* no intrinsic support in OpenSIPS for 64bit integers -
+					 * converting to string */
+					s = int2str(bson_iter_int64(it), &len);
+					s[len]=0;
+					if (type == BSON_TYPE_DOCUMENT) {
+						json_object_object_add(obj,curr_key,json_object_new_string(s));
+					} else if (type == BSON_TYPE_ARRAY) {
+						json_object_array_add(obj,json_object_new_string(s));
+					}
+					break;
+				case BSON_TYPE_DOUBLE:
+					/* no intrinsic support in OpenSIPS for floating point numbers
+					 * converting to int */
+					LM_DBG("Found key %s with type double\n",curr_key);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,
+								json_object_new_int((int)bson_iter_double(it)));
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,
+						       json_object_new_int((int)bson_iter_double(it)));
+					break;
+				case BSON_TYPE_UTF8:
+					LM_DBG("Found key %s with type string\n",curr_key);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,
+								json_object_new_string(bson_iter_utf8(it, NULL)));
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,json_object_new_string(bson_iter_utf8(it, NULL)));
+					break;
+				case BSON_TYPE_BOOL:
+					LM_DBG("Found key %s with type bool\n",curr_key);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,
+								json_object_new_int((int)bson_iter_bool(it)));
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,json_object_new_int((int)bson_iter_bool(it)));
+					break;
+				case BSON_TYPE_DATE_TIME:
+					LM_DBG("Found key %s with type date\n",curr_key);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,
+								json_object_new_int((int)(bson_iter_date_time(it)/1000)));
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,json_object_new_int((int)(bson_iter_date_time(it)/1000)));
+					break;
+				case BSON_TYPE_ARRAY:
+					LM_DBG("Found key %s with type array\n",curr_key);
+					obj2 = json_object_new_array();
+					bson_iter_recurse(it, &it2);
+					bson_to_json_generic(obj2,&it2,BSON_TYPE_ARRAY);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,obj2);
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,obj2);
+					break;
+				case BSON_TYPE_DOCUMENT:
+					LM_DBG("Found key %s with type object\n",curr_key);
+					obj2 = json_object_new_object();
+					bson_iter_recurse(it, &it2);
+					bson_to_json_generic(obj2,&it2,BSON_TYPE_DOCUMENT);
+					if (type == BSON_TYPE_DOCUMENT)
+						json_object_object_add(obj,curr_key,obj2);
+					else if (type == BSON_TYPE_ARRAY)
+						json_object_array_add(obj,obj2);
+					break;
+				case BSON_TYPE_OID:
+					memset(oid, 0, sizeof oid);
+					bson_oid_to_string(bson_iter_oid(it), oid);
+					json_object_object_add(obj,curr_key,
+							json_object_new_string(oid));
+					LM_DBG(" Found type %d for key %s \n",
+							bson_iter_type(it),curr_key);
+					break;
+				case BSON_TYPE_NULL:
+						json_object_object_add(obj,curr_key,NULL);
+					break;
+				default:
+					LM_WARN("Unsupported type %d for key %s - skipping\n",
+							bson_iter_type(it),curr_key);
+		}
+	}
+}
diff --git a/modules/cachedb_mongodb2/cachedb_mongodb_json.h b/modules/cachedb_mongodb2/cachedb_mongodb_json.h
new file mode 100644
index 0000000..f3bcbd9
--- /dev/null
+++ b/modules/cachedb_mongodb2/cachedb_mongodb_json.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2011-2017 OpenSIPS Project
+ *
+ * This file is part of opensips, a free SIP server.
+ *
+ * opensips is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * opensips is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
+ */
+
+#ifndef CACHEDBMONGO_JSON_H
+#define CACHEDBMONGO_JSON_H
+
+#include "cachedb_mongodb_dbase.h"
+
+#include <bson.h>
+#include <stdint.h>
+
+int json_to_bson(char *json,bson_t *bb);
+
+void bson_to_json_generic(struct json_object *obj, bson_iter_t *it,
+                          bson_type_t type);
+
+#endif /* CACHEDBMONGO_JSON_H */
+
diff --git a/modules/cachedb_mongodb2/doc/cachedb_mongodb.xml b/modules/cachedb_mongodb2/doc/cachedb_mongodb.xml
new file mode 100644
index 0000000..c9e026c
--- /dev/null
+++ b/modules/cachedb_mongodb2/doc/cachedb_mongodb.xml
@@ -0,0 +1,48 @@
+<?xml version="1.0" encoding='ISO-8859-1'?>
+<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
+"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd" [
+
+
+<!ENTITY admin SYSTEM "cachedb_mongodb_admin.xml">
+
+<!-- Include general documentation entities -->
+<!ENTITY % docentities SYSTEM "../../../doc/entities.xml">
+%docentities;
+
+]>
+
+<book>
+<bookinfo>
+<title>cachedb_mongodb Module</title>
+<productname class="trade">&osipsname;</productname>
+<authorgroup>
+<author>
+<firstname>Vladut-Stefan</firstname>
+<surname>Paiu</surname>
+<affiliation><orgname>OpenSIPS Solutions</orgname></affiliation>
+<address>
+<email>vladpaiu@opensips.org</email>
+<otheraddr>
+	&osipssol;
+</otheraddr>
+</address>
+</author>
+<editor>
+<firstname>Vladut-Stefan</firstname>
+<surname>Paiu</surname>
+<address>
+<email>vladpaiu@opensips.org</email>
+</address>
+</editor>
+</authorgroup>
+<copyright>
+<year>2013-2017</year>
+<holder>&osipssol;</holder>
+</copyright>
+</bookinfo>
+<toc></toc>
+
+&admin;
+
+</book>
+
diff --git a/modules/cachedb_mongodb2/doc/cachedb_mongodb_admin.xml b/modules/cachedb_mongodb2/doc/cachedb_mongodb_admin.xml
new file mode 100644
index 0000000..43f0687
--- /dev/null
+++ b/modules/cachedb_mongodb2/doc/cachedb_mongodb_admin.xml
@@ -0,0 +1,327 @@
+<!-- Module User's Guide -->
+
+<chapter>
+	
+	<title>&adminguide;</title>
+	
+	<section>
+	<title>Overview</title>
+	<para>
+		This module is an implementation of a cache system designed to work with
+		MongoDB servers.
+		It implements the Key-Value interface exposed by the OpenSIPS core.
+	</para>
+	<para>
+		The underlying client library is compatible with any of the following
+		MongoDB server versions: 2.4, 2.6, 3.0, 3.2 and 3.4, as stated in 
+		<ulink url="https://docs.mongodb.com/ecosystem/drivers/driver-compatibility-reference/">
+			the MongoDB documentation</ulink>.
+	</para>
+	<para>
+	</para>
+	</section>
+
+	<section>
+	<title>Advantages</title>
+	<para>
+		<itemizedlist>
+			<listitem>
+			<para>
+				<emphasis>memory costs are no longer on the server
+				</emphasis>
+			</para>
+			</listitem>
+
+			<listitem>
+			<para>
+				<emphasis>many servers can be used inside a cluster, so the memory
+				is virtually unlimited</emphasis>
+			</para>
+			</listitem>
+
+			<listitem>
+			<para>
+				<emphasis>the cache is 100% persistent. A restart
+					of OpenSIPS server will not affect the DB. The MongoDB is also
+				persistent so it can also be restarted without loss of information.</emphasis>
+			</para>
+			</listitem>
+
+			<listitem>
+			<para>
+				<emphasis>MongoDB is an open-source project so
+				it can be used to exchange data
+				 with various other applications</emphasis>
+			</para>
+			</listitem>
+
+			<listitem>
+			<para>
+				<emphasis>By creating a MongoDB Cluster, multiple OpenSIPS
+				instances can easily share key-value information</emphasis>
+			</para>
+			</listitem>
+
+			<listitem>
+			<para>
+				<emphasis>This module also implements the CacheDB Raw query
+				capability, thus you can run whatever query that the MongoDB
+				back-end supports, taking full advatange of it.
+				</emphasis>
+			</para>
+			</listitem>
+
+		</itemizedlist>
+	</para>
+	<para>
+	</para>
+	</section>
+
+	<section>
+	<title>Limitations</title>
+	
+		
+		<para>
+			<itemizedlist>
+			<listitem>
+			<para>
+			<emphasis>
+		keys (in key:value pairs) may not contain spaces or control characters
+		</emphasis>
+			</para>
+			</listitem>
+
+		</itemizedlist>
+		</para>
+	</section>
+
+	<section>
+	<title>Dependencies</title>
+	<section>
+		<title>&osips; Modules</title>
+		<para>
+		None.
+		</para>
+	</section>
+	
+	<section>
+		<title>External Libraries or Applications</title>
+		<para>
+		The following packages must be installed before running &osips; with this module loaded:
+		</para>
+		<para>
+		<example>
+			<title>Runtime requirements for "cachedb_mongodb"</title>
+				<programlisting format="linespecific">
+# Debian / Ubuntu
+sudo apt-get install libjson-c2 libmongoc-1.0
+
+# Red Hat / CentOS
+sudo yum install json-c mongo-c-driver
+				</programlisting>
+			</example>
+		</para>
+
+		<para>
+		The following packages are required in order to compile this module:
+		</para>
+		<para>
+		<example>
+			<title>Compilation requirements for "cachedb_mongodb"</title>
+				<programlisting format="linespecific">
+# Debian / Ubuntu
+sudo apt-get install libjson-c-dev libmongoc-dev libbson-dev
+
+# Red Hat / CentOS
+sudo yum install json-c-devel mongo-c-driver-devel
+				</programlisting>
+			</example>
+		</para>
+
+	</section>
+	</section>
+
+	<section>
+		<title>Exported Parameters</title>
+		<section>
+		<title><varname>cachedb_url</varname> (string)</title>
+		<para>
+			The URLs of the server groups that OpenSIPS will connect to in order
+			to allow the cache_store(), cache_fetch(), etc. functions to be used
+			from the OpenSIPS script. It can be set more than one time.
+			The prefix part of the URL will be the identifier that will be used
+			from the script.
+		</para>
+
+		<para>
+			The URL syntax is identical to the one used by MongoDB, including
+			connect string options. For more info,
+			please refer to <ulink url="https://docs.mongodb.com/manual/reference/connection-string/">
+				the official MongoDB connect string documentation</ulink>.
+		</para>
+
+		<example>
+		<title>Set <varname>cachedb_url</varname> parameter</title>
+		<programlisting format="linespecific">
+...
+modparam("cachedb_mongodb", "cachedb_url","mongodb:instance1://localhost:27017/db.collection")
+modparam("cachedb_mongodb", "cachedb_url","mongodb:replicaset1://1.2.3.4:27017,2.3.4.5:27017,3.4.5.6:27017/db.collection?replicaSet=test")
+...
+	</programlisting>
+		</example>
+
+		<example>
+		<title>Use MongoDB servers </title>
+		<programlisting format="linespecific">
+...
+cache_store("mongodb:group1", "key", "$ru value");
+cache_fetch("mongodb:replicaset1", "key", $avp(10));
+cache_remove("mongodb:cluster1", "key");
+...
+	</programlisting>
+		</example>
+	</section>
+
+		<section>
+		<title><varname>exec_threshold</varname> (int)</title>
+		<para>
+			The maximum number of microseconds that a mongodb query can last.
+			Anything above the threshold will trigger a warning message to the log
+		</para>
+		<para>
+		<emphasis>Default value is <quote>0 ( unlimited - no warnings )</quote>.
+		</emphasis>
+		</para>
+		<example>
+		<title>Set <varname>exec_threshold</varname> parameter</title>
+		<programlisting format="linespecific">
+...
+modparam("cachedb_mongodb", "exec_threshold", 100000)
+...
+	</programlisting>
+		</example>
+	</section>
+
+		<section>
+		<title><varname>compat_mode_2.4</varname> (int)</title>
+		<para>
+			Switch the module into compatibility mode for MongoDB 2.4 servers.
+			Specifically, this allows "insert/update/delete" raw queries to not fail,
+			since they were introduced in MongoDB 2.6. The module will interpret
+			the raw query JSON, convert it to its corresponding command and run it.
+		</para>
+		<para>
+			Caveat: only the minimally required raw query options are
+			supported in this mode.
+		</para>
+		<para>
+		<emphasis>Default value is <quote>0 (disabled)</quote>.
+		</emphasis>
+		</para>
+		<example>
+		<title>Setting the <varname>compat_mode_2.4</varname> parameter</title>
+		<programlisting format="linespecific">
+...
+modparam("cachedb_mongodb", "compat_mode_2.4", 1)
+...
+	</programlisting>
+		</example>
+	</section>
+
+		<section>
+		<title><varname>compat_mode_3.0</varname> (int)</title>
+		<para>
+			Switch the module into compatibility mode for MongoDB 2.6/3.0 servers.
+			Specifically, this allows "find" raw queries to not fail,
+			since they were introduced in MongoDB 3.2. The module will interpret
+			the "find" raw query JSON, convert it to its corresponding command and run it.
+		</para>
+		<para>
+			Caveat: only the minimally required options for "find" raw queries are
+			supported in this mode.
+		</para>
+		<para>
+		<emphasis>Default value is <quote>0 (disabled)</quote>.
+		</emphasis>
+		</para>
+		<example>
+		<title>Setting the <varname>compat_mode_3.0</varname> parameter</title>
+		<programlisting format="linespecific">
+...
+modparam("cachedb_mongodb", "compat_mode_3.0", 1)
+...
+	</programlisting>
+		</example>
+	</section>
+
+</section>
+
+	<section>
+		<title>Exported Functions</title>
+		<para>The module does not export functions to be used
+		in configuration script.</para>
+	</section>	
+
+
+	<section>
+	<title>Raw Query Syntax</title>
+		<para>
+			The cachedb_mongodb module supports raw queries, thus taking
+			full advantage of the capabilities of the back-end, including
+			query-specific options such as read/write preference, timeouts,
+			filtering options, etc.
+		</para>
+		<para>
+			The query syntax is identical to the mongo cli. Documentation for it
+			can be found on the
+			<ulink url="https://docs.mongodb.com/manual/reference/command/nav-crud/">
+				MongoDB website</ulink>. Query results
+			are returned as JSON documents, that one can further process
+			in the OpenSIPS script by using the JSON module.
+		</para>
+
+		<para>
+			Some example raw queries:
+			<example>
+			<title>MongoDB Raw Insert</title>
+			<programlisting format="linespecific">
+...
+cache_raw_query("mongodb:cluster", "{ \
+    \"insert\": \"ip_blacklist\", \
+    \"documents\": [{ \
+        \"username\": \"$fU\", \
+        \"ip\": \"$si\", \
+        \"attempts\": 1 \
+     }]}",
+ "$avp(out)");
+xlog("INSERT RAW QUERY returned $rc, output: '$avp(out)'\n");
+...
+			</programlisting>
+			</example>
+
+			<example>
+			<title>MongoDB Raw Update</title>
+			<programlisting format="linespecific">
+...
+cache_raw_query("mongodb:cluster", "{ \
+    \"update\": \"ip_blacklist\", \
+    \"updates\": [{ \
+        \"q\": { \
+            \"username\": \"$fU\", \
+            \"ip\": \"$si\" \
+         }, \
+        \"u\": { \
+            \"$$inc\": {\"attempts\": 1} \
+         } \
+      }]}",
+ "$avp(out)");
+xlog("UPDATE RAW QUERY returned $rc, output: '$avp(out)'\n");
+...
+			</programlisting>
+			</example>
+		</para>
+
+	</section>
+
+</chapter>
+
-- 
1.8.3.1

