From 54e42b96931eccda2b3461a624abb845c8ec1483 Mon Sep 17 00:00:00 2001
From: Varadhan M <varadhan@cem-solutions.net>
Date: Tue, 12 Jan 2016 16:56:07 +0530
Subject: [PATCH] Fixed asm for armv7l arch

---
 fastlock.h |   12 ++++++++----
 1 file changed, 8 insertions(+), 4 deletions(-)

diff --git a/fastlock.h b/fastlock.h
index 8866e40..116a2a3 100644
--- a/fastlock.h
+++ b/fastlock.h
@@ -100,10 +100,14 @@ inline static int tsl(fl_lock_t* lock)
 
 #elif defined __CPU_arm
 	asm volatile(
-			"# here \n\t"
-			"swpb %0, %1, [%2] \n\t"
-			: "=&r" (val)
-			: "r"(1), "r" (lock) : "memory"
+			"   ldrex %0, [%2] \n\t" 
+			"   cmp %0, #0 \n\t"
+			"   it eq \n\t"
+			"   strexeq %0, %3, [%2] \n\t" /* executed only if Z=1 */
+			/* if %0!=0 => either it was 1 initially or was 0
+			 * and somebody changed it just before the strexeq (so the 
+			 * lock is taken) => it's safe to return %0 */
+			: "=&r"(val), "=m"(*lock) : "r"(lock), "r"(1) : "cc"
 	);
 
 #elif defined(__CPU_ppc) || defined(__CPU_ppc64)
-- 
1.7.9.5

